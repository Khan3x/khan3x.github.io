

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.png">
  <link rel="icon" href="/img/logo.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="toutou">
  <meta name="keywords" content="">
  
    <meta name="description" content="CS Concepts本章笔记着重对CS相关专业科目的名词进行解释，在修考问答题和面试八股文场景下适用。这些名词都来源于CS相关学科。本篇笔记内容按科目划分，每个科目下面有对应的常见高频名词解释（中&#x2F;英）。 1. Data Stucture &amp; AlgorithmsHash TableA Hash Table is a data structure that maps keys">
<meta property="og:type" content="article">
<meta property="og:title" content="CS Concepts">
<meta property="og:url" content="http://toutou.zeabur.app/2025/02/09/CS-Concepts/index.html">
<meta property="og:site_name" content="偷偷星球">
<meta property="og:description" content="CS Concepts本章笔记着重对CS相关专业科目的名词进行解释，在修考问答题和面试八股文场景下适用。这些名词都来源于CS相关学科。本篇笔记内容按科目划分，每个科目下面有对应的常见高频名词解释（中&#x2F;英）。 1. Data Stucture &amp; AlgorithmsHash TableA Hash Table is a data structure that maps keys">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/hash.jpg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/hash2.jpg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/pnp.jpg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/tsp.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/bnb.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/btree.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/astar.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/minimax.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/process.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/dynamic_branch.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/parallelism.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/wb.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/multicache.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/cache3.jpg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/superscalar.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/tlb.svg">
<meta property="og:image" content="http://toutou.zeabur.app/img/concepts/dht.svg">
<meta property="article:published_time" content="2025-02-09T15:25:17.000Z">
<meta property="article:modified_time" content="2025-07-06T16:01:41.042Z">
<meta property="article:author" content="toutou">
<meta property="article:tag" content="修考">
<meta property="article:tag" content="CS Concepts">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://toutou.zeabur.app/img/concepts/hash.jpg">
  
  
  
  <title>CS Concepts - 偷偷星球</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"toutou.zeabur.app","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>偷偷星球</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/wall.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="CS Concepts"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-02-09 23:25" pubdate>
          February 9, 2025 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.3k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          28 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">CS Concepts</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="CS-Concepts"><a href="#CS-Concepts" class="headerlink" title="CS Concepts"></a>CS Concepts</h1><p>本章笔记着重对CS相关专业科目的名词进行解释，在修考问答题和面试八股文场景下适用。这些名词都来源于CS相关学科。本篇笔记内容按科目划分，每个科目下面有对应的常见高频名词解释（中&#x2F;英）。</p>
<h2 id="1-Data-Stucture-Algorithms"><a href="#1-Data-Stucture-Algorithms" class="headerlink" title="1. Data Stucture &amp; Algorithms"></a>1. Data Stucture &amp; Algorithms</h2><h3 id="Hash-Table"><a href="#Hash-Table" class="headerlink" title="Hash Table"></a>Hash Table</h3><p>A Hash Table is a data structure that maps keys to values using a hash function. It stores data in an array-like structure, where each key is hashed to an index. Collisions (when different keys map to the same index) are handled using techniques like chaining or open addressing. Hash tables provide average O(1) time complexity for insertion, deletion, and lookup. They are widely used in databases, caching, and symbol tables.<br><img src="/img/concepts/hash.jpg" srcset="/img/loading.gif" lazyload alt="Hash Table" style="max-width: 70%; height: auto;" /></p>
<hr>
<h3 id="Hash-Collision-Hash-Clash"><a href="#Hash-Collision-Hash-Clash" class="headerlink" title="Hash Collision &#x2F; Hash Clash"></a>Hash Collision &#x2F; Hash Clash</h3><p>A <strong>hash collision (hash clash)</strong> occurs when two different inputs produce the same hash value in a hash function. Since hash functions map a large input space to a smaller output space, collisions are inevitable due to the <strong>pigeonhole principle</strong>. Collisions can weaken security in cryptographic hashes (e.g., MD5, SHA-1) and reduce efficiency in hash tables. Techniques like <strong>chaining, open addressing, and better hash functions</strong> help mitigate collisions. Stronger cryptographic hashes (e.g., SHA-256) minimize the risk of intentional hash clashes (collision attacks).</p>
<hr>
<h3 id="Open-Addressing-Closed-Hashing"><a href="#Open-Addressing-Closed-Hashing" class="headerlink" title="Open Addressing &#x2F; Closed Hashing"></a>Open Addressing &#x2F; Closed Hashing</h3><p><strong>Open addressing</strong> is a collision resolution technique in hash tables where all elements are stored directly in the table without external chaining. When a collision occurs, the algorithm searches for the next available slot using a probing sequence (e.g., <strong>linear probing, quadratic probing, or double hashing</strong>). <strong>Linear probing</strong> checks the next slot sequentially, <strong>quadratic probing</strong> uses a quadratic function to find slots, and <strong>double hashing</strong> applies a second hash function for probing. Open addressing avoids linked lists, reducing memory overhead but may suffer from clustering. It works best when the load factor is kept low to maintain efficient lookups.</p>
<hr>
<h3 id="Seperate-Chaining"><a href="#Seperate-Chaining" class="headerlink" title="Seperate Chaining"></a>Seperate Chaining</h3><p><strong>Separate chaining</strong> is a collision resolution technique in hash tables where each bucket stores multiple values using a linked list (or another data structure like a BST). When a collision occurs, the new element is simply added to the linked list at that index. This method allows the table to handle an unlimited number of collisions but increases memory usage. Performance depends on the length of the chains; with a well-distributed hash function, the average lookup time remains <strong>O(1) in best case</strong> and <strong>O(n) in worst case</strong>. <strong>Rehashing</strong> or using a larger table can help maintain efficiency.</p>
<img src="/img/concepts/hash2.jpg" srcset="/img/loading.gif" lazyload alt="Hash Table" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="P-≠-NP"><a href="#P-≠-NP" class="headerlink" title="P ≠ NP"></a>P ≠ NP</h3><p><strong>P ≠ NP</strong> means that not all problems whose solutions can be <em>verified quickly</em> (in polynomial time) can also be <em>solved quickly</em>.<br><strong>P</strong> is the class of problems that can be <strong>solved</strong> in polynomial time.<br><strong>NP</strong> is the class of problems whose solutions can be <strong>verified</strong> in polynomial time.<br>The question is: if a problem’s solution can be verified quickly, can it also be found quickly?<br>Most experts believe <strong>P ≠ NP</strong>, but it has not been proven yet.</p>
<img src="/img/concepts/pnp.jpg" srcset="/img/loading.gif" lazyload alt="Hash Table" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="NP-Hard-Problems"><a href="#NP-Hard-Problems" class="headerlink" title="NP Hard Problems"></a>NP Hard Problems</h3><p>NP-Hard problems are computational problems that are at least as difficult as the hardest problems in NP (nondeterministic polynomial time). Solving an NP-Hard problem quickly would mean we could solve every NP problem quickly too, but no such efficient solution is known. These problems may not even have verifiable solutions in polynomial time. They often involve optimization or decision-making with many possibilities, like scheduling, routing, or packing.Examples include the Traveling Salesman Problem and Knapsack Problem.</p>
<hr>
<h3 id="NP-Complete-Problems"><a href="#NP-Complete-Problems" class="headerlink" title="NP-Complete Problems"></a>NP-Complete Problems</h3><p><strong>NP-Complete problems</strong> are a special class of problems that are both:</p>
<ol>
<li><strong>In NP</strong> – their solutions can be verified in polynomial time, and</li>
<li><strong>NP-Hard</strong> – as hard as the hardest problems in NP.</li>
</ol>
<p>If you can solve any NP-Complete problem quickly (in polynomial time), you can solve <em>all</em> NP problems quickly. Famous examples include the <strong>Boolean Satisfiability Problem (SAT)</strong> and <strong>Traveling Salesman Problem (decision version)</strong>.</p>
<hr>
<h3 id="The-Travelling-Salesman-Problem"><a href="#The-Travelling-Salesman-Problem" class="headerlink" title="The Travelling Salesman Problem"></a>The Travelling Salesman Problem</h3><p>The <strong>Travelling Salesman Problem (TSP)</strong> asks for the shortest possible route that visits each city once and returns to the starting city.<br>It is a classic <strong>combinatorial optimization</strong> problem in computer science and operations research.<br>TSP is <strong>NP-hard</strong>, meaning there’s no known efficient algorithm to solve all cases quickly.<br>Exact solutions use methods like <strong>brute force</strong>, <strong>dynamic programming</strong>, or <strong>branch and bound</strong>.<br>Approximation and heuristic algorithms (e.g. genetic algorithms, simulated annealing) are used for large instances.</p>
<img src="/img/concepts/tsp.svg" srcset="/img/loading.gif" lazyload alt="TSP" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="The-Knapsack-Problem"><a href="#The-Knapsack-Problem" class="headerlink" title="The Knapsack Problem"></a>The Knapsack Problem</h3><p>The <strong>Knapsack Problem</strong> asks how to choose items with given weights and values to maximize total value without exceeding a weight limit.<br>Each item can be either included or excluded (0-1 Knapsack).<br>It’s a classic <strong>NP-hard</strong> optimization problem.<br>Dynamic programming is commonly used for exact solutions.<br>Greedy or approximation methods are used for large instances.</p>
<hr>
<h3 id="The-SAT-Boolean-Satisfiability-problem"><a href="#The-SAT-Boolean-Satisfiability-problem" class="headerlink" title="The SAT (Boolean Satisfiability) problem"></a>The SAT (Boolean Satisfiability) problem</h3><p>The <strong>SAT (Boolean Satisfiability)</strong> problem asks whether there exists an assignment of true&#x2F;false values to variables that makes a Boolean formula true.<br>It’s the <strong>first problem proven to be NP-complete</strong>.<br>The formula is usually given in <strong>CNF (Conjunctive Normal Form)</strong>.<br>SAT solvers use techniques like backtracking and clause learning.<br>Many real-world problems (e.g. planning, verification) can be reduced to SAT.</p>
<hr>
<h3 id="Divide-and-Conquer"><a href="#Divide-and-Conquer" class="headerlink" title="Divide and Conquer"></a>Divide and Conquer</h3><p><strong>Divide and Conquer</strong> is a problem-solving strategy that works in three main steps:</p>
<ol>
<li><strong>Divide</strong> the problem into smaller sub-problems of the same type.</li>
<li><strong>Conquer</strong> each sub-problem by solving them recursively.</li>
<li><strong>Combine</strong> the solutions of sub-problems to get the final result.</li>
</ol>
<p>Classic examples include <strong>Merge Sort</strong>, <strong>Quick Sort</strong>, and <strong>Binary Search</strong>.</p>
<hr>
<h3 id="Branch-and-Bound"><a href="#Branch-and-Bound" class="headerlink" title="Branch and Bound"></a>Branch and Bound</h3><p>Branch and Bound is an algorithmic paradigm for solving combinatorial optimization problems efficiently. It systematically divides the solution space into smaller subproblems (branching) and computes bounds to eliminate unpromising branches early (pruning). The method maintains an upper bound from feasible solutions and a lower bound from relaxed problems, allowing it to discard branches that cannot contain the optimal solution. This approach significantly reduces the search space compared to exhaustive enumeration, making it effective for problems like integer programming, traveling salesman, and knapsack problems.</p>
<img src="/img/concepts/bnb.svg" srcset="/img/loading.gif" lazyload alt="Branch and Bound" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="B-tree"><a href="#B-tree" class="headerlink" title="B-tree"></a>B-tree</h3><p>A <strong>B-tree</strong> is a self-balancing search tree that maintains sorted data for efficient insertion, deletion, and search in <strong>O(log n)</strong> time.<br>It allows each node to have <strong>multiple keys and children</strong>, making it wider and shallower than binary trees.<br>All leaf nodes are at the same level, ensuring the tree stays balanced.<br>It’s widely used in <strong>databases and file systems</strong> for fast disk-based access.</p>
<img src="/img/concepts/btree.svg" srcset="/img/loading.gif" lazyload alt="Hash Table" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="Heap-Sort"><a href="#Heap-Sort" class="headerlink" title="Heap Sort"></a>Heap Sort</h3><p><strong>Heap Sort</strong> is a comparison-based sorting algorithm that uses a <strong>binary heap</strong> data structure. It works in two main steps:</p>
<ol>
<li><strong>Build a Max Heap</strong> from the input data so that the largest element is at the root.</li>
<li><strong>Extract the root (maximum element)</strong>, swap it with the last item, reduce the heap size, and <strong>heapify</strong> the root to maintain the max heap. Repeat until the heap is empty.</li>
</ol>
<p>Time Complexity: <strong>Best, Average, Worst:</strong> O(n log n)</p>
<p>Key Characteristics:</p>
<ul>
<li><strong>In-place</strong> sorting (no extra space needed)</li>
<li><strong>Not stable</strong></li>
<li>Good for scenarios where memory is limited and performance is important.</li>
</ul>
<hr>
<h3 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h3><p><strong>Merge Sort</strong> is a <strong>divide and conquer</strong> sorting algorithm that divides the input array into smaller parts, sorts them, and then merges the sorted parts.</p>
<p>Steps:</p>
<ol>
<li><strong>Divide</strong> the array into two halves.</li>
<li><strong>Recursively sort</strong> each half.</li>
<li><strong>Merge</strong> the two sorted halves into one sorted array.</li>
</ol>
<p>Time Complexity:</p>
<ul>
<li><strong>Best, Average, Worst:</strong> O(n log n)</li>
</ul>
<p>Key Characteristics:</p>
<ul>
<li><strong>Stable</strong> sort</li>
<li><strong>Not in-place</strong> (uses extra space for merging)</li>
<li>Great for sorting linked lists or large datasets with consistent performance.</li>
</ul>
<hr>
<h3 id="Quick-Sort"><a href="#Quick-Sort" class="headerlink" title="Quick Sort"></a>Quick Sort</h3><p><strong>Quick Sort</strong> is a <strong>divide and conquer</strong> sorting algorithm that works by selecting a <strong>pivot</strong> element and partitioning the array into two parts:</p>
<p>Steps:</p>
<ol>
<li><strong>Choose a pivot</strong> (e.g., first, last, or random element).</li>
<li><strong>Partition</strong> the array: elements less than pivot go left, greater go right.</li>
<li><strong>Recursively apply</strong> Quick Sort to the left and right parts.</li>
</ol>
<p>Time Complexity:</p>
<ul>
<li><strong>Best &amp; Average:</strong> O(n log n)</li>
<li><strong>Worst (unbalanced partition):</strong> O(n²)</li>
</ul>
<p>Key Characteristics:</p>
<ul>
<li><strong>In-place</strong></li>
<li><strong>Not stable</strong></li>
<li>Very fast in practice with good pivot selection</li>
</ul>
<hr>
<h3 id="A-Algorithm"><a href="#A-Algorithm" class="headerlink" title="A* Algorithm"></a>A* Algorithm</h3><p>The A* algorithm finds the shortest path by combining actual cost from the start (g) and estimated cost to the goal (h).<br>It selects nodes with the lowest total cost <code>f(n) = g(n) + h(n)</code>.<br>It uses a priority queue to explore the most promising paths first.<br>The heuristic <code>h(n)</code> must not overestimate to ensure optimality.<br>It’s widely used in games, maps, and AI for efficient pathfinding.</p>
<img src="/img/concepts/astar.svg" srcset="/img/loading.gif" lazyload alt="A* Algorithm" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="Minimax-Algorithm"><a href="#Minimax-Algorithm" class="headerlink" title="Minimax Algorithm"></a>Minimax Algorithm</h3><p>The <strong>Minimax algorithm</strong> is used in two-player games to find the optimal move by assuming both players play optimally.<br>It recursively explores all possible moves, maximizing the player’s score and minimizing the opponent’s.<br>“Max” tries to get the highest score; “Min” tries to get the lowest.<br>The game tree is evaluated using a scoring function at terminal states.<br>It’s often optimized with <strong>alpha-beta pruning</strong> to skip unnecessary branches.</p>
<img src="/img/concepts/minimax.svg" srcset="/img/loading.gif" lazyload alt="Minimax Algorithm" style="max-width: 100%; height: auto;" />

<hr>
<h2 id="2-Operating-System"><a href="#2-Operating-System" class="headerlink" title="2. Operating System"></a>2. Operating System</h2><h3 id="Procss-Thread"><a href="#Procss-Thread" class="headerlink" title="Procss &amp; Thread"></a>Procss &amp; Thread</h3><p>A process is the basic unit of resource allocation and scheduling in an operating system, with its own independent address space and resources. A thread is the basic unit of CPU scheduling, and a process can contain multiple threads that share the process’s memory and resources. Threads switch faster and have lower overhead, making them suitable for concurrent execution. In contrast, processes are independent of each other, with higher switching overhead but greater stability. While multithreading improves execution efficiency, it also introduces challenges such as synchronization and mutual exclusion.</p>
<img src="/img/concepts/process.svg" srcset="/img/loading.gif" lazyload alt="Process & Thread" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h3><p>A <strong>semaphore</strong> in an operating system is a synchronization tool used to manage <strong>concurrent access to shared resources</strong> by multiple processes or threads.</p>
<p><strong>Key Points:</strong></p>
<ul>
<li><p>It is an <strong>integer variable</strong> that controls access based on its value.</p>
</li>
<li><p>There are two main operations:</p>
<ul>
<li><strong>wait (P)</strong>: Decreases the semaphore value. If the result is negative, the process is blocked.</li>
<li><strong>signal (V)</strong>: Increases the semaphore value. If there are blocked processes, one is unblocked.</li>
</ul>
</li>
</ul>
<p><strong>Types:</strong></p>
<ol>
<li><strong>Binary Semaphore</strong>: Takes values 0 or 1, similar to a mutex.</li>
<li><strong>Counting Semaphore</strong>: Can take non-negative integer values, used to control access to a resource with multiple instances.</li>
</ol>
<p><strong>Use Case:</strong></p>
<p>Semaphores help <strong>avoid race conditions</strong>, <strong>deadlocks</strong>, and ensure <strong>mutual exclusion</strong> in critical sections.</p>
<p>Example use: Controlling access to a printer shared by multiple processes.</p>
<hr>
<h3 id="Critical-Section"><a href="#Critical-Section" class="headerlink" title="Critical Section"></a>Critical Section</h3><p>A <strong>Critical Section</strong> in an operating system is a part of a program where a <strong>shared resource</strong> (like a variable, file, or device) is accessed. Since shared resources can be corrupted if accessed by multiple processes or threads simultaneously, <strong>only one process should enter the critical section at a time</strong>.</p>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Mutual Exclusion</strong>: Ensures that only one process is in the critical section at any given time.</li>
<li><strong>Entry Section</strong>: Code that requests entry to the critical section.</li>
<li><strong>Exit Section</strong>: Code that signals the process is leaving the critical section.</li>
<li><strong>Remainder Section</strong>: All other code outside the critical section.</li>
</ul>
<blockquote>
<p>Goals of Critical Section Management:</p>
</blockquote>
<ol>
<li><strong>Mutual Exclusion</strong> – Only one process in the critical section at a time.</li>
<li><strong>Progress</strong> – If no process is in the critical section, one of the waiting processes should be allowed to enter.</li>
<li><strong>Bounded Waiting</strong> – A process should not wait forever to enter the critical section.</li>
</ol>
<p><strong>Tools Used:</strong></p>
<ul>
<li><strong>Semaphores</strong></li>
<li><strong>Mutexes</strong></li>
<li><strong>Monitors</strong></li>
<li><strong>Locks</strong></li>
</ul>
<p>These tools help implement and manage access to critical sections safely.</p>
<hr>
<h2 id="3-Computer-Architecture"><a href="#3-Computer-Architecture" class="headerlink" title="3. Computer Architecture"></a>3. Computer Architecture</h2><h3 id="Pipeline-hazard"><a href="#Pipeline-hazard" class="headerlink" title="Pipeline hazard"></a>Pipeline hazard</h3><p>A <strong>pipeline hazard</strong> in computer architecture refers to a situation that <strong>prevents the next instruction in the pipeline from executing at its expected time</strong>, causing delays.</p>
<p><strong>Types of Pipeline Hazards:</strong></p>
<ol>
<li><p><strong>Data Hazard</strong>:<br>Occurs when instructions <strong>depend on the result of a previous instruction</strong> that hasn’t completed yet.<br>Example:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-pgsql" data-language="pgsql"><code class="language-pgsql">ADD R1, R2, R3  
SUB R4, R1, R5  ← depends on R1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>
</li>
<li><p><strong>Control Hazard</strong> (Branch Hazard):<br>Happens when the pipeline makes <strong>wrong predictions about instruction flow</strong>, such as branches or jumps.</p>
</li>
<li><p><strong>Structural Hazard</strong>:<br>Arises when <strong>hardware resources are insufficient</strong> to support all instructions in parallel (e.g., one memory unit shared by two stages).</p>
</li>
</ol>
<p><strong>Solution Techniques:</strong></p>
<ul>
<li><strong>Forwarding (data hazard)</strong></li>
<li><strong>Stalling (inserting bubbles)</strong></li>
<li><strong>Branch prediction (control hazard)</strong></li>
<li><strong>Adding hardware units (structural hazard)</strong></li>
</ul>
<p>Pipeline hazards reduce performance and must be carefully handled in modern CPUs.</p>
<hr>
<h3 id="Data-hazard"><a href="#Data-hazard" class="headerlink" title="Data hazard"></a>Data hazard</h3><p>A <strong>data hazard</strong> occurs in a pipelined processor when an instruction depends on the <strong>result of a previous instruction</strong> that has not yet completed, causing a conflict in data access.</p>
<p><strong>Types of Data Hazards:</strong></p>
<ol>
<li><p><strong>RAW (Read After Write)</strong> – Most common<br>An instruction needs to <strong>read</strong> a register that a previous instruction will <strong>write</strong>, but the write hasn’t happened yet.<br>Example:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-pgsql" data-language="pgsql"><code class="language-pgsql">ADD R1, R2, R3  
SUB R4, R1, R5  ← needs R1 before it&#39;s written<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>
</li>
<li><p><strong>WAR (Write After Read)</strong> – Rare in simple pipelines<br>A later instruction writes to a register <strong>before</strong> an earlier instruction reads it.</p>
</li>
<li><p><strong>WAW (Write After Write)</strong> – Happens in out-of-order execution<br>Two instructions write to the same register in the wrong order.</p>
</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li><strong>Forwarding (bypassing)</strong> – Pass result directly to the next instruction.</li>
<li><strong>Stalling</strong> – Delay the dependent instruction until data is ready.</li>
</ul>
<p>Data hazards can slow down pipeline performance if not properly managed.</p>
<hr>
<h3 id="Control-hazard"><a href="#Control-hazard" class="headerlink" title="Control hazard"></a>Control hazard</h3><p>A <strong>control hazard</strong> (also called a <strong>branch hazard</strong>) occurs in pipelined processors when the <strong>flow of instruction execution changes</strong>, typically due to <strong>branch or jump instructions</strong>.</p>
<p><strong>Cause:</strong></p>
<p>The processor <strong>doesn’t know early enough</strong> whether a branch will be taken, so it may <strong>fetch the wrong instructions</strong>.</p>
<p><strong>Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-assembly" data-language="assembly"><code class="language-assembly">BEQ R1, R2, LABEL   ; Branch if R1 &#x3D;&#x3D; R2
ADD R3, R4, R5      ; May be wrongly fetched if branch is taken<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>

<p><strong>Solutions:</strong></p>
<ul>
<li><strong>Branch Prediction</strong> – Guess whether the branch will be taken.</li>
<li><strong>Branch Delay Slot</strong> – Always execute the instruction after the branch.</li>
<li><strong>Pipeline Flushing</strong> – Discard wrongly fetched instructions.</li>
<li><strong>Early Branch Resolution</strong> – Move branch decision to earlier pipeline stage.</li>
</ul>
<p>Control hazards can reduce pipeline efficiency by introducing <strong>stalls or flushes</strong>.</p>
<hr>
<h3 id="Structural-hazard"><a href="#Structural-hazard" class="headerlink" title="Structural hazard"></a>Structural hazard</h3><p>A <strong>structural hazard</strong> occurs in a pipelined processor when <strong>two or more instructions compete for the same hardware resource</strong> at the same time, and the hardware <strong>cannot handle all of them simultaneously</strong>.</p>
<p><strong>Example:</strong></p>
<p>If the CPU has <strong>one memory unit</strong> shared for both <strong>instruction fetch</strong> and <strong>data access</strong>, a conflict arises when:</p>
<ul>
<li>One instruction is being <strong>fetched</strong>, and</li>
<li>Another instruction needs to <strong>read&#x2F;write data</strong> from memory at the same time.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li><strong>Add more hardware resources</strong> (e.g. separate instruction and data memory – like in Harvard architecture)</li>
<li><strong>Stall one of the instructions</strong> to resolve the conflict</li>
</ul>
<p>Structural hazards are <strong>less common</strong> in modern CPUs due to better hardware design but can still occur in resource-limited systems.</p>
<hr>
<h3 id="Dynamic-Branch-Prediction"><a href="#Dynamic-Branch-Prediction" class="headerlink" title="Dynamic Branch Prediction"></a>Dynamic Branch Prediction</h3><p><strong>Dynamic Branch Prediction</strong> is a technique used in modern CPUs to <strong>predict the outcome of branch instructions (e.g., if-else, loops) at runtime</strong>, based on the <strong>history of previous executions</strong>.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Learns from past behavior</strong> of branches.</li>
<li><strong>Updates prediction</strong> as the program runs.</li>
<li>More accurate than static prediction (which always predicts taken&#x2F;not taken).</li>
</ul>
<p><strong>Common Methods:</strong></p>
<ol>
<li><p><strong>1-bit predictor</strong>:<br>Remembers the last outcome (taken or not taken).</p>
</li>
<li><p><strong>2-bit predictor</strong>:<br>More stable; uses a state machine to change prediction only after two mispredictions.</p>
</li>
<li><p><strong>Branch History Table (BHT)</strong>:<br>Stores past branch outcomes and uses them for prediction.</p>
</li>
<li><p><strong>Global History &amp; Pattern History Table (PHT)</strong>:<br>Tracks patterns of multiple branches for more accuracy (used in <strong>two-level predictors</strong>).</p>
</li>
</ol>
<p><strong>Benefit:</strong></p>
<p>Improves <strong>pipeline efficiency</strong> by reducing <strong>control hazards</strong> and minimizing <strong>stall cycles</strong> caused by branch mispredictions.</p>
<img src="/img/concepts/dynamic_branch.svg" srcset="/img/loading.gif" lazyload alt="Dynamic Branch Prediction" style="max-width: 100%; height: auto;" />

<p>–</p>
<h3 id="Instruction-level-Parallelsim"><a href="#Instruction-level-Parallelsim" class="headerlink" title="Instruction-level Parallelsim"></a>Instruction-level Parallelsim</h3><p><strong>Instruction-Level Parallelism (ILP)</strong> refers to the ability of a CPU to <strong>execute multiple instructions simultaneously</strong> during a single clock cycle.</p>
<p><strong>Key Idea:</strong></p>
<p>Many instructions in a program are <strong>independent</strong> and can be executed in <strong>parallel</strong>, rather than strictly one after another.</p>
<p><strong>Types of ILP:</strong></p>
<ol>
<li><p><strong>Compiler-Level ILP (Static ILP)</strong><br>The <strong>compiler rearranges instructions</strong> at compile time to exploit parallelism (e.g., instruction scheduling).</p>
</li>
<li><p><strong>Hardware-Level ILP (Dynamic ILP)</strong><br>The <strong>CPU detects parallelism at runtime</strong>, using features like:</p>
<ul>
<li><strong>Pipelining</strong></li>
<li><strong>Superscalar execution</strong> (multiple execution units)</li>
<li><strong>Out-of-order execution</strong></li>
<li><strong>Speculative execution</strong></li>
</ul>
</li>
</ol>
<p><strong>Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-assembly" data-language="assembly"><code class="language-assembly">ADD R1, R2, R3  
MUL R4, R5, R6   ; Independent → can run in parallel with ADD<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>

<p><strong>Benefits:</strong></p>
<ul>
<li>Increases CPU performance <strong>without increasing clock speed</strong></li>
<li>Makes better use of CPU resources</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li><strong>Data, control, and structural hazards</strong> limit ILP</li>
<li><strong>Dependence between instructions</strong> reduces parallelism</li>
</ul>
<p>Modern processors heavily rely on ILP for high-speed performance.</p>
<img src="/img/concepts/parallelism.svg" srcset="/img/loading.gif" lazyload alt="Instruction-level Parallelsim" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="Clock-frequency"><a href="#Clock-frequency" class="headerlink" title="Clock frequency"></a>Clock frequency</h3><p><strong>Clock frequency</strong> (also called <strong>clock speed</strong>) is the rate at which a processor executes instructions, measured in <strong>Hertz (Hz)</strong> — typically in <strong>gigahertz (GHz)</strong> for modern CPUs.</p>
<p><strong>Key Points:</strong></p>
<ul>
<li><strong>1 GHz &#x3D; 1 billion cycles per second</strong></li>
<li>Each <strong>clock cycle</strong> is a tick of the CPU’s internal clock, during which it can perform basic operations (like fetch, decode, execute).</li>
<li>A <strong>higher clock frequency</strong> generally means the CPU can <strong>perform more operations per second</strong>.</li>
</ul>
<blockquote>
<p>Example: A CPU with <strong>3.0 GHz</strong> can perform <strong>3 billion clock cycles per second</strong>.</p>
</blockquote>
<p>Higher clock speed <strong>doesn’t always mean better performance</strong>, because:</p>
<ul>
<li>Other factors like <strong>Instruction-Level Parallelism (ILP)</strong>, <strong>number of cores</strong>, <strong>cache</strong>, and <strong>architecture efficiency</strong> also matter.</li>
<li>Very high clock speeds can cause <strong>more heat and power consumption</strong>.</li>
</ul>
<p><strong>Clock frequency &#x3D; speed of instruction processing</strong>, but <strong>not the only factor</strong> in CPU performance.</p>
<hr>
<h3 id="Register-renaming"><a href="#Register-renaming" class="headerlink" title="Register renaming"></a>Register renaming</h3><p><strong>Register renaming</strong> is a technique used in modern CPUs to <strong>eliminate false data dependencies</strong> (also called <em>name dependencies</em>) between instructions, allowing for more <strong>instruction-level parallelism (ILP)</strong> and better performance.</p>
<blockquote>
<p>Why It’s Needed?</p>
</blockquote>
<p>In pipelined or out-of-order execution, instructions may appear dependent due to <strong>using the same register name</strong>, even when there’s <strong>no real data dependency</strong>.</p>
<p>There are two main false dependencies:</p>
<ol>
<li><strong>Write After Write (WAW)</strong></li>
<li><strong>Write After Read (WAR)</strong></li>
</ol>
<p><strong>Example (with false dependency):</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-assembly" data-language="assembly"><code class="language-assembly">1. ADD R1, R2, R3  
2. SUB R1, R4, R5  ← falsely depends on instruction 1 (WAW)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>

<p>Both write to <code>R1</code>, but they’re unrelated operations. Register renaming removes this conflict.</p>
<blockquote>
<p>How It Works:</p>
</blockquote>
<ul>
<li>The CPU maintains a <strong>larger set of physical registers</strong> than the number of logical (visible) registers.</li>
<li>It dynamically assigns <strong>different physical registers</strong> to each instruction, even if they use the same logical name.</li>
<li>This avoids name-based conflicts, enabling <strong>out-of-order</strong> and <strong>parallel execution</strong>.</li>
</ul>
<p>Benefits:</p>
<ul>
<li><strong>Eliminates false dependencies</strong></li>
<li><strong>Increases parallelism</strong></li>
<li><strong>Improves CPU throughput</strong></li>
</ul>
<p>Summary:</p>
<p><strong>Register renaming</strong> helps CPUs <strong>run more instructions in parallel</strong> by resolving unnecessary register name conflicts, boosting performance.</p>
<hr>
<h3 id="Sign-Magnitude-Representation-原码"><a href="#Sign-Magnitude-Representation-原码" class="headerlink" title="Sign-Magnitude Representation (原码)"></a>Sign-Magnitude Representation (原码)</h3><p><strong>Sign-Magnitude Representation (原码)</strong> is a binary method for representing signed integers.</p>
<ul>
<li><strong>Sign bit</strong>: The most significant bit indicates the sign — <code>0</code> for positive, <code>1</code> for negative.</li>
<li><strong>Magnitude bits</strong>: The remaining bits represent the absolute value of the number in binary.</li>
</ul>
<p><strong>Example (using 8 bits):</strong></p>
<ul>
<li><code>+5</code> in sign-magnitude: <code>00000101</code></li>
<li><code>-5</code> in sign-magnitude: <code>10000101</code></li>
</ul>
<p><strong>Features:</strong></p>
<ul>
<li>Symmetrical representation for positive and negative numbers</li>
<li>Has <strong>two representations of zero</strong>: <code>00000000</code> (+0) and <code>10000000</code> (−0)</li>
</ul>
<p>Since arithmetic operations with sign-magnitude require handling the sign separately, it is less efficient in hardware compared to two’s complement.</p>
<hr>
<h3 id="One’s-Complement-反码"><a href="#One’s-Complement-反码" class="headerlink" title="One’s Complement (反码)"></a><strong>One’s Complement (反码)</strong></h3><p><strong>One’s Complement (反码)</strong> is a binary method for representing signed integers.</p>
<ul>
<li><strong>Positive numbers</strong>: Same as in regular binary.</li>
<li><strong>Negative numbers</strong>: Invert all bits of the positive number (i.e., change 0 to 1 and 1 to 0).</li>
</ul>
<p><strong>Example (using 8 bits):</strong></p>
<ul>
<li><code>+5</code>: <code>00000101</code></li>
<li><code>-5</code>: <code>11111010</code> (one’s complement of <code>00000101</code>)</li>
</ul>
<p><strong>Features:</strong></p>
<ul>
<li>Two representations of zero: <code>00000000</code> (+0) and <code>11111111</code> (−0)</li>
<li>Subtraction can be done using addition, but still needs end-around carry handling.</li>
</ul>
<p>Less efficient than two’s complement for arithmetic operations.</p>
<hr>
<h3 id="Two’s-Complement-补码"><a href="#Two’s-Complement-补码" class="headerlink" title="Two’s Complement (补码)"></a>Two’s Complement (补码)</h3><p><strong>Two’s Complement (补码)</strong> is the most common binary method for representing signed integers.</p>
<ul>
<li><strong>Positive numbers</strong>: Same as regular binary.</li>
<li><strong>Negative numbers</strong>: Invert all bits of the positive number (get the one’s complement), then add 1.</li>
</ul>
<p><strong>Example (using 8 bits):</strong></p>
<ul>
<li><code>+5</code>: <code>00000101</code></li>
<li><code>-5</code>: <code>11111011</code> (one’s complement of <code>00000101</code> is <code>11111010</code>, plus 1 gives <code>11111011</code>)</li>
</ul>
<p><strong>Features:</strong></p>
<ul>
<li>Only <strong>one zero</strong>: <code>00000000</code></li>
<li>Arithmetic operations (addition and subtraction) are simple and efficient</li>
<li>Widely used in modern computers for signed integer representation</li>
</ul>
<hr>
<h3 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h3><p><strong>Benchmark</strong> refers to a <strong>standardized test or reference</strong> used to evaluate the performance or quality of a system, device, program, or product.</p>
<p>A <strong>benchmark</strong> is a method of assessing the performance of an object by running a set of standard tests and comparing the results to others. It helps measure efficiency and identify areas for improvement.</p>
<p><strong>Common Use Cases</strong></p>
<ol>
<li><p><strong>Computer Hardware:</strong></p>
<ul>
<li>Benchmarking CPUs, GPUs, SSDs, etc., by running performance tests and comparing scores.</li>
<li>Common tools: Cinebench, Geekbench, PCMark.</li>
</ul>
</li>
<li><p><strong>Software Development:</strong></p>
<ul>
<li>Measuring the speed or efficiency of code or algorithms to optimize performance.</li>
</ul>
</li>
<li><p><strong>Business Management:</strong></p>
<ul>
<li>Comparing business performance to industry best practices to find room for improvement.</li>
</ul>
</li>
</ol>
<hr>
<h3 id="Big-endian"><a href="#Big-endian" class="headerlink" title="Big-endian"></a>Big-endian</h3><p><strong>Big-endian</strong> is a type of <strong>byte order</strong> where the <strong>most significant byte (MSB)</strong> is stored at the <strong>lowest memory address</strong>, and the least significant byte is stored at the highest address.</p>
<p>Suppose we have a 32-bit hexadecimal number: <code>0x12345678</code></p>
<p>In <strong>Big-endian</strong> format, it would be stored in memory like this (from low to high address):</p>
<table>
<thead>
<tr>
<th>Address</th>
<th>Byte Value</th>
</tr>
</thead>
<tbody><tr>
<td>0x00</td>
<td>0x12</td>
</tr>
<tr>
<td>0x01</td>
<td>0x34</td>
</tr>
<tr>
<td>0x02</td>
<td>0x56</td>
</tr>
<tr>
<td>0x03</td>
<td>0x78</td>
</tr>
</tbody></table>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Commonly used in <strong>network protocols</strong> (e.g., TCP&#x2F;IP), hence also known as <strong>network byte order</strong>.</li>
<li>The opposite of Big-endian is <strong>Little-endian</strong>, where the least significant byte comes first.</li>
</ul>
<hr>
<h3 id="Little-endian-Brief-Explanation"><a href="#Little-endian-Brief-Explanation" class="headerlink" title="Little-endian (Brief Explanation)"></a>Little-endian (Brief Explanation)</h3><p><strong>Little-endian</strong> is a type of <strong>byte order</strong> where the <strong>least significant byte (LSB)</strong> is stored at the <strong>lowest memory address</strong>, and the most significant byte is stored at the highest address.</p>
<p>Suppose we have a 32-bit hexadecimal number: <code>0x12345678</code></p>
<p>In <strong>Little-endian</strong> format, it would be stored in memory like this (from low to high address):</p>
<table>
<thead>
<tr>
<th>Address</th>
<th>Byte Value</th>
</tr>
</thead>
<tbody><tr>
<td>0x00</td>
<td>0x78</td>
</tr>
<tr>
<td>0x01</td>
<td>0x56</td>
</tr>
<tr>
<td>0x02</td>
<td>0x34</td>
</tr>
<tr>
<td>0x03</td>
<td>0x12</td>
</tr>
</tbody></table>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Commonly used in <strong>x86 architecture</strong> (Intel and AMD CPUs).</li>
<li>It is the <strong>opposite of Big-endian</strong>, which stores the most significant byte first.</li>
<li>Easier for some CPUs to handle arithmetic operations on varying byte lengths.</li>
</ul>
<hr>
<h3 id="Temporal-Locality-时间局部性"><a href="#Temporal-Locality-时间局部性" class="headerlink" title="Temporal Locality (时间局部性)"></a>Temporal Locality (时间局部性)</h3><p><strong>Temporal locality</strong> is a common program behavior in computer systems that means:</p>
<blockquote>
<p><strong>“If a data item is accessed, it is likely to be accessed again in the near future.”</strong></p>
</blockquote>
<p>Consider the following code snippet:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    sum <span class="token operator">+=</span> array<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div></figure>


<ul>
<li>When <code>array[0]</code> is accessed, the program will soon access <code>array[1]</code>, <code>array[2]</code>, etc.</li>
<li>The variable <code>sum</code> is also accessed repeatedly.</li>
<li>This demonstrates <strong>temporal locality</strong>.</li>
</ul>
<p><strong>Additional Notes:</strong></p>
<ul>
<li><p><strong>Caches</strong> take advantage of temporal locality to improve performance.</p>
</li>
<li><p>Temporal locality commonly appears in:</p>
<ul>
<li>Loop variables</li>
<li>Frequently called functions</li>
<li>Intermediate computation results</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Spatial-Locality-空间局部性"><a href="#Spatial-Locality-空间局部性" class="headerlink" title="Spatial Locality (空间局部性)"></a>Spatial Locality (空间局部性)</h3><p><strong>Spatial locality</strong> is a common program behavior in computer systems that means:</p>
<blockquote>
<p><strong>“If a data item is accessed, nearby data items (in memory) are likely to be accessed soon.”</strong></p>
</blockquote>
<p>Consider the following code snippet:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    sum <span class="token operator">+=</span> array<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div></figure>

<ul>
<li>When <code>array[0]</code> is accessed, the program will likely access <code>array[1]</code>, <code>array[2]</code>, …, <code>array[99]</code> shortly after.</li>
<li>These elements are stored in <strong>adjacent memory locations</strong>, so this illustrates <strong>spatial locality</strong>.</li>
</ul>
<p><strong>Additional Notes:</strong></p>
<ul>
<li><p><strong>Caches</strong> use spatial locality by loading not just a single data item but also <strong>nearby memory blocks</strong> (called a cache line).</p>
</li>
<li><p>Spatial locality commonly appears in:</p>
<ul>
<li>Sequential array access</li>
<li>Traversal of linked lists or other data structures with contiguous memory</li>
<li>Instruction fetches during linear code execution</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Hardware-Prefetching-a-k-a-Prefetch"><a href="#Hardware-Prefetching-a-k-a-Prefetch" class="headerlink" title="Hardware Prefetching (a.k.a. Prefetch)"></a>Hardware Prefetching (a.k.a. Prefetch)</h3><p><strong>Hardware Prefetching</strong> is a technique where the CPU automatically predicts and loads data into the cache <strong>before</strong> it is actually needed.<br>It helps reduce memory latency by exploiting <strong>spatial</strong> and <strong>temporal locality</strong>.<br>For example, if a program accesses memory sequentially, the hardware may prefetch the next few addresses.<br>Prefetched data is stored in the cache, making future access faster.<br>This improves overall CPU performance, especially in data-intensive workloads.</p>
<hr>
<h3 id="Write-Through（写直达）"><a href="#Write-Through（写直达）" class="headerlink" title="Write Through（写直达）"></a>Write Through（写直达）</h3><p><strong>Write Through</strong> is a <strong>cache writing policy</strong> used in computer architecture to manage how data is written to the memory hierarchy, specifically between the <strong>cache</strong> and <strong>main memory</strong>.</p>
<p><strong>🔧 Definition:</strong><br>In a <strong>Write Through</strong> policy, <strong>every write operation</strong> to the cache is <strong>immediately and simultaneously written to main memory</strong>. This ensures that the main memory always holds the most up-to-date data.</p>
<p><strong>✅ Advantages:</strong></p>
<ul>
<li><strong>Data consistency</strong>: Main memory always reflects the latest data written by the CPU.</li>
<li><strong>Simple to implement</strong>: Because memory and cache are always in sync, it simplifies memory coherence in multi-processor systems.</li>
</ul>
<p><strong>❌ Disadvantages:</strong></p>
<ul>
<li><strong>Slower write performance</strong>: Every write operation must access main memory, which is slower than just writing to cache.</li>
<li><strong>Higher memory traffic</strong>: Frequent memory writes can increase bus usage and reduce overall system performance.</li>
</ul>
<p><strong>🧠 Example:</strong><br>If a CPU writes value <code>42</code> to memory address <code>0xA0</code>:</p>
<ul>
<li>The value is written to <strong>L1 cache</strong>.</li>
<li>At the same time, the value is also written to <strong>main memory</strong>.</li>
</ul>
<p><strong>💡 Related Concept:</strong></p>
<ul>
<li>Compare with <strong>Write Back</strong> policy, where updates are only made to the cache and written to memory <strong>later</strong>, often when the cache block is replaced.</li>
</ul>
<p><strong>Summary:</strong><br>Write Through provides <strong>strong consistency</strong> between cache and memory at the cost of <strong>write speed and efficiency</strong>.</p>
<hr>
<h3 id="Write-Back（写回）"><a href="#Write-Back（写回）" class="headerlink" title="Write Back（写回）"></a>Write Back（写回）</h3><p><strong>Write Back</strong> is a <strong>cache writing policy</strong> used in computer architecture to manage how data is written between the <strong>CPU cache</strong> and <strong>main memory</strong>.</p>
<p><strong>🔧 Definition:</strong><br>In a <strong>Write Back</strong> policy, data is written <strong>only to the cache</strong> at first. The updated data is written to <strong>main memory only when the cache block is replaced</strong> (i.e., evicted). Until then, the main memory may hold stale data.</p>
<p><strong>✅ Advantages:</strong></p>
<ul>
<li><strong>Faster write performance</strong>: Since writes are done only in cache, it reduces the latency of write operations.</li>
<li><strong>Reduced memory traffic</strong>: Multiple writes to the same memory location are performed only once to main memory when the block is evicted.</li>
</ul>
<p><strong>❌ Disadvantages:</strong></p>
<ul>
<li><strong>Data inconsistency risk</strong>: Main memory may not reflect the latest data, which complicates memory coherence in multi-core systems.</li>
<li><strong>More complex control logic</strong>: Needs a <strong>dirty bit</strong> to track whether a cache block has been modified.</li>
</ul>
<p><strong>🧠 Example:</strong><br>If the CPU writes value <code>42</code> to memory address <code>0xA0</code>:</p>
<ul>
<li>The value is written <strong>only to the cache</strong> (marked as dirty).</li>
<li>When the cache block containing <code>0xA0</code> is later evicted, <strong>then</strong> the value is written to <strong>main memory</strong>.</li>
</ul>
<p><strong>💡 Related Concept:</strong></p>
<ul>
<li>Compare with <strong>Write Through</strong>, where every write updates both the cache and main memory <strong>simultaneously</strong>.</li>
</ul>
<p><strong>Summary:</strong><br>Write Back improves <strong>write efficiency and performance</strong>, but introduces <strong>complexity and consistency challenges</strong>.</p>
<img src="/img/concepts/wb.svg" srcset="/img/loading.gif" lazyload alt="Write Back VS Write Through" style="max-width: 100%; height: auto;" />


<table>
<thead>
<tr>
<th>特性</th>
<th>写回（Write Back）</th>
<th>写直达（Write Through）</th>
</tr>
</thead>
<tbody><tr>
<td>写入位置</td>
<td>只写缓存（先缓存，后主存）</td>
<td>同时写入缓存和主存</td>
</tr>
<tr>
<td>写入主存时机</td>
<td>缓存块被替换时才写回主存</td>
<td>每次写操作都立即写入主存</td>
</tr>
<tr>
<td>性能</td>
<td>高性能，减少主存访问，延迟更低</td>
<td>写入慢，因频繁访问主存</td>
</tr>
<tr>
<td>主存数据一致性</td>
<td>复杂，需要使用“脏位”等一致性机制管理</td>
<td>简单，主存总是最新数据</td>
</tr>
<tr>
<td>系统总线负载</td>
<td>较低，减少写操作带来的总线流量</td>
<td>较高，写操作总是同步主存，增加总线压力</td>
</tr>
<tr>
<td>缓存一致性维护</td>
<td>较困难，多核系统中需额外一致性协议支持</td>
<td>较简单，主存作为权威数据源</td>
</tr>
<tr>
<td>适用场景</td>
<td>写操作频繁、对性能要求高的系统</td>
<td>对一致性要求高、系统结构简单的场景</td>
</tr>
</tbody></table>
<hr>
<h3 id="Multicache-Multi-level-Cache"><a href="#Multicache-Multi-level-Cache" class="headerlink" title="Multicache (Multi-level Cache)"></a>Multicache (Multi-level Cache)</h3><p><strong>Multicache</strong>, or <strong>Multi-level Cache</strong>, refers to a hierarchical(分层的) caching system used in modern CPUs to bridge the speed gap between the fast processor and the slower main memory. It typically consists of <strong>multiple levels of cache</strong>, such as <strong>L1</strong>, <strong>L2</strong>, and <strong>L3</strong>, each with different sizes, speeds, and purposes.</p>
<p><strong>🔧 Definition:</strong> A <strong>multi-level cache</strong> system includes:</p>
<ul>
<li><strong>L1 Cache</strong>: Closest to the CPU core, smallest and fastest.</li>
<li><strong>L2 Cache</strong>: Larger than L1, slower, and may be shared or private per core.</li>
<li><strong>L3 Cache</strong>: Even larger and slower, typically shared across all CPU cores.</li>
</ul>
<p>This layered structure allows faster access to frequently used data while reducing latency and improving CPU efficiency.</p>
<p><strong>🏗️ Structure Example:</strong></p>
<table>
<thead>
<tr>
<th>Level</th>
<th>Size</th>
<th>Speed</th>
<th>Shared</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td>L1</td>
<td>~32KB</td>
<td>Very fast</td>
<td>No</td>
<td>Immediate access for the CPU</td>
</tr>
<tr>
<td>L2</td>
<td>~256KB</td>
<td>Fast</td>
<td>Maybe</td>
<td>Secondary buffer</td>
</tr>
<tr>
<td>L3</td>
<td>~8MB</td>
<td>Slower</td>
<td>Yes</td>
<td>Shared cache for all cores</td>
</tr>
</tbody></table>
<p><strong>✅ Advantages:</strong></p>
<ul>
<li><strong>Improved performance</strong>: Reduces average memory access time.</li>
<li><strong>Lower latency</strong>: L1 and L2 caches are much faster than main memory.</li>
<li><strong>Better scalability</strong>: Helps in multi-core systems where data sharing and access times are critical.</li>
</ul>
<p><strong>❌ Disadvantages:</strong></p>
<ul>
<li><strong>Complex design</strong>: Managing coherence and consistency across multiple levels is challenging.</li>
<li><strong>Increased cost and power</strong>: More hardware and logic are required.</li>
<li><strong>Cache misses</strong>: Still possible, especially if working sets exceed cache sizes.</li>
</ul>
<p><strong>💡 Notes:</strong></p>
<ul>
<li>Most modern processors use <strong>inclusive（包容）</strong>, <strong>exclusive（排他）</strong>, or <strong>non-inclusive（非包容）</strong> cache strategies to determine how data is stored across cache levels.</li>
<li>Multi-level cache systems often work in tandem with <strong>cache coherence protocols</strong> like MESI in multi-core CPUs.</li>
</ul>
<p><strong>📌 Summary</strong></p>
<p><strong>Multicache systems</strong> provide a hierarchical buffer between the CPU and main memory, optimizing performance and efficiency by leveraging fast, small caches for frequently accessed data and larger, slower caches for broader access.</p>
<img src="/img/concepts/multicache.svg" srcset="/img/loading.gif" lazyload alt="multicache" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="Direct-Mapping（直接映射）"><a href="#Direct-Mapping（直接映射）" class="headerlink" title="Direct Mapping（直接映射）"></a>Direct Mapping（直接映射）</h3><p><strong>Direct Mapping</strong> is a simple cache mapping technique used in computer architecture to determine where a memory block will be placed in the cache.</p>
<p><strong>🔧 Definition:</strong><br>In <strong>Direct Mapping</strong>, each block of main memory maps to <strong>exactly one</strong> cache line. The mapping is usually done using:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">Cache Line Index &#x3D; (Main Memory Block Address) mod (Number of Cache Lines)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p>This makes it easy and fast to locate data in the cache, but can lead to many <strong>conflicts</strong> if multiple blocks map to the same cache line.</p>
<p><strong>✅ Advantages:</strong></p>
<ul>
<li><strong>Simple and fast</strong> implementation</li>
<li><strong>Low hardware complexity</strong></li>
</ul>
<p><strong>❌ Disadvantages:</strong></p>
<ul>
<li><strong>High conflict rate</strong>: Two different memory blocks mapping to the same line will continuously evict each other</li>
<li><strong>Low flexibility</strong> compared to other mapping techniques (e.g., set-associative)</li>
</ul>
<p><strong>🧠 Example:</strong><br>If a cache has 8 lines (0 to 7), and a block from main memory has address 24:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">Cache Line &#x3D; 24 mod 8 &#x3D; 0<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p>So this block will always be placed in cache line 0.</p>
<p><strong>Summary:</strong><br>Direct Mapping provides fast and simple cache access, but is prone to frequent conflicts.</p>
<hr>
<h3 id="Set-Associative-Mapping（组相连映射）"><a href="#Set-Associative-Mapping（组相连映射）" class="headerlink" title="Set- Associative Mapping（组相连映射）"></a>Set- Associative Mapping（组相连映射）</h3><p><strong>Set-Associative Mapping</strong> is a cache mapping technique that combines the benefits of both <strong>Direct Mapping</strong> and <strong>Fully Associative Mapping</strong> to balance performance and complexity.</p>
<p><strong>🔧 Definition:</strong><br>In <strong>Set-Associative Mapping</strong>, the cache is divided into multiple <strong>sets</strong>, and each set contains <strong>multiple cache lines (ways)</strong>. A block from main memory maps to <strong>exactly one set</strong>, but <strong>can be placed in any line (way) within that set</strong>.</p>
<p>The set is selected using:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">Set Index &#x3D; (Main Memory Block Address) mod (Number of Sets)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p>Within the set, placement and replacement follow policies like <strong>Least Recently Used (LRU)</strong> or <strong>Random</strong>.</p>
<p><strong>✅ Advantages:</strong></p>
<ul>
<li><strong>Lower conflict rate</strong> than Direct Mapping</li>
<li><strong>More flexible</strong> placement within sets</li>
<li><strong>Good balance</strong> between speed and cache hit rate</li>
</ul>
<p><strong>❌ Disadvantages:</strong></p>
<ul>
<li><strong>More complex</strong> hardware than Direct Mapping</li>
<li><strong>Slightly slower</strong> lookup due to checking multiple lines in a set</li>
</ul>
<p><strong>🧠 Example:</strong><br>If a cache has <strong>4 sets</strong>, each with <strong>2 lines (2-way set-associative)</strong>, and a memory block with address 12:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">Set Index &#x3D; 12 mod 4 &#x3D; 0<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p>The block can be placed in <strong>either of the 2 lines in Set 0</strong>.</p>
<p><strong>Summary:</strong><br>Set-Associative Mapping provides a <strong>balanced approach</strong> with better conflict resolution than Direct Mapping and <strong>less complexity</strong> than Fully Associative Mapping.</p>
<hr>
<h3 id="Full-Associative-Mapping（全相联映射）"><a href="#Full-Associative-Mapping（全相联映射）" class="headerlink" title="Full Associative Mapping（全相联映射）"></a>Full Associative Mapping（全相联映射）</h3><p><strong>Full Associative Mapping</strong> is a cache mapping technique in which <strong>a memory block can be placed in any cache line</strong>, offering maximum flexibility and minimal conflict.</p>
<p><strong>🔧 Definition:</strong><br>In <strong>Full Associative Mapping</strong>, there are <strong>no restrictions</strong> on where a block can be placed in the cache. Any block from main memory can be stored in <strong>any cache line</strong>.</p>
<p>To find a block, the cache must <strong>search all lines</strong> using <strong>comparators</strong> that match tags.</p>
<p><strong>✅ Advantages:</strong></p>
<ul>
<li><strong>No conflict misses</strong> (except when cache is full)</li>
<li><strong>Best flexibility</strong> in placement</li>
<li><strong>Highest cache hit potential</strong></li>
</ul>
<p><strong>❌ Disadvantages:</strong></p>
<ul>
<li><strong>Complex hardware</strong>: Requires searching the entire cache in parallel</li>
<li><strong>Slower access time</strong> due to tag comparisons</li>
<li><strong>Expensive</strong> in terms of power and chip area</li>
</ul>
<p><strong>🧠 Example:</strong><br>If a cache has <strong>8 lines</strong>, and a block from memory has address <code>0x2F</code>, it can be placed in <strong>any one</strong> of the 8 lines. When checking for a hit, the cache must compare the block’s tag with the tags of <strong>all 8 lines</strong>.</p>
<p><strong>Summary:</strong><br>Full Associative Mapping provides <strong>maximum flexibility</strong> and <strong>minimal conflict</strong>, but comes with <strong>higher hardware cost</strong> and <strong>slower lookup times</strong>.</p>
<hr>
<h3 id="Comparison-of-Cache-Mapping-Techniques"><a href="#Comparison-of-Cache-Mapping-Techniques" class="headerlink" title="Comparison of Cache Mapping Techniques"></a>Comparison of Cache Mapping Techniques</h3><table>
<thead>
<tr>
<th>Feature</th>
<th>Direct Mapping</th>
<th>Set-Associative Mapping</th>
<th>Full Associative Mapping</th>
</tr>
</thead>
<tbody><tr>
<td>Placement Rule</td>
<td>Each block maps to <strong>one line</strong></td>
<td>Each block maps to <strong>one set</strong>, can go into <strong>any line</strong> in that set</td>
<td>Each block can go into <strong>any line</strong> in the cache</td>
</tr>
<tr>
<td>Flexibility</td>
<td>Low</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td>Hardware Complexity</td>
<td>Low (simplest)</td>
<td>Medium</td>
<td>High (most complex)</td>
</tr>
<tr>
<td>Access Speed</td>
<td>Fast</td>
<td>Slightly slower</td>
<td>Slowest (due to parallel comparisons)</td>
</tr>
<tr>
<td>Conflict Misses</td>
<td>High</td>
<td>Lower than Direct Mapping</td>
<td>None (except capacity misses)</td>
</tr>
<tr>
<td>Replacement Policy</td>
<td>Not needed (only one line)</td>
<td>Needed within each set (e.g., LRU)</td>
<td>Needed for the entire cache (e.g., LRU)</td>
</tr>
<tr>
<td>Cost (Power&#x2F;Area)</td>
<td>Low</td>
<td>Moderate</td>
<td>High</td>
</tr>
<tr>
<td>Use Case Suitability</td>
<td>Simple, low-power systems</td>
<td>General-purpose CPUs</td>
<td>High-performance or critical systems</td>
</tr>
</tbody></table>
<img src="/img/concepts/cache3.jpg" srcset="/img/loading.gif" lazyload alt="三种cache映射" style="max-width: 100%; height: auto;" />



<hr>
<h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h3><p><strong>Cache</strong> is a small, high-speed memory that stores frequently used data to speed up access. It sits between the CPU and main memory to reduce latency. It enables fast program access to frequently used addresses.</p>
<hr>
<h3 id="Non-blocking-Cache（非阻塞缓存）"><a href="#Non-blocking-Cache（非阻塞缓存）" class="headerlink" title="Non-blocking Cache（非阻塞缓存）"></a>Non-blocking Cache（非阻塞缓存）</h3><p>A <strong>Non-blocking Cache</strong> allows the CPU to continue processing other instructions <strong>while waiting for a cache miss</strong> to be resolved.<br>It supports <strong>multiple outstanding memory requests</strong> simultaneously.<br>This improves overall system performance by reducing CPU idle time.<br>Non-blocking caches are especially useful in <strong>out-of-order execution</strong> and <strong>superscalar processors</strong>.<br>They often use structures like <strong>Miss Status Handling Registers (MSHRs)</strong> to track pending requests.</p>
<p><strong>非阻塞缓存</strong>允许 CPU 在<strong>等待缓存未命中（Cache Miss）处理完成的同时继续执行其他指令</strong>。<br>它支持<strong>多个未完成的内存请求</strong>同时存在。<br>这通过减少 CPU 空闲时间来提升系统整体性能。<br>非阻塞缓存在<strong>乱序执行</strong>和<strong>超标量处理器</strong>中尤为重要。<br>它通常使用如 <strong>未命中状态处理寄存器（Miss Status Handling Registers, MSHRs）</strong> 的结构来追踪未完成的请求。</p>
<hr>
<h3 id="Superscalar-Architecture（超标量架构）"><a href="#Superscalar-Architecture（超标量架构）" class="headerlink" title="Superscalar Architecture（超标量架构）"></a>Superscalar Architecture（超标量架构）</h3><p><strong>Superscalar Architecture</strong> is a CPU design that allows the processor to <strong>fetch, decode, and execute multiple instructions simultaneously</strong> during each clock cycle.<br>It achieves this by using <strong>multiple pipelines</strong> and <strong>parallel execution units</strong> (e.g., ALUs, FPUs).<br>This architecture increases <strong>instruction-level parallelism (ILP)</strong>, boosting performance without raising the clock speed.<br>Superscalar CPUs include features like <strong>out-of-order execution</strong>, <strong>register renaming</strong>, and <strong>branch prediction</strong> to handle dependencies and control flow efficiently.<br>Examples of superscalar processors include modern Intel and AMD CPUs.</p>
<img src="/img/concepts/superscalar.svg" srcset="/img/loading.gif" lazyload alt="超标量架构" style="max-width: 100%; height: auto;" />






<hr>
<h3 id="🔍-TLB-Translation-Lookaside-Buffer"><a href="#🔍-TLB-Translation-Lookaside-Buffer" class="headerlink" title="🔍 TLB (Translation Lookaside Buffer)"></a>🔍 TLB (Translation Lookaside Buffer)</h3><p><strong>TLB</strong> stands for <strong>Translation Lookaside Buffer</strong>. It is a small, fast cache used in the memory management unit (MMU) of a computer’s CPU to improve the speed of <strong>virtual-to-physical address translation</strong>.</p>
<p><strong>📌 Why is TLB needed?</strong></p>
<p>When a CPU accesses memory using a virtual address, it must be translated into a physical address using the <strong>page table</strong>. This translation is time-consuming. The TLB stores recent translations, so if the virtual address has been accessed recently, the translation can be retrieved quickly without accessing the full page table.</p>
<p><strong>✅ Key Characteristics</strong></p>
<ul>
<li><strong>Cache for page table entries</strong></li>
<li><strong>Reduces page table access time</strong></li>
<li>Typically contains <strong>64 to 512 entries</strong></li>
<li>Can be <strong>fully associative</strong>, <strong>set-associative</strong>, or <strong>direct-mapped</strong></li>
</ul>
<p><strong>📈 How it works</strong></p>
<ol>
<li><strong>CPU generates a virtual address</strong></li>
<li><strong>MMU checks the TLB</strong> for the virtual page number (VPN)</li>
<li>If found (<strong>TLB hit</strong>): use the cached physical page number (PPN)</li>
<li>If not found (<strong>TLB miss</strong>): access the page table, then update the TLB</li>
</ol>
<p><strong>🧠 TLB Hit vs TLB Miss</strong></p>
<table>
<thead>
<tr>
<th>Event</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>TLB Hit</td>
<td>The virtual address is found in the TLB → Fast address translation</td>
</tr>
<tr>
<td>TLB Miss</td>
<td>The virtual address is not found in the TLB → Page table lookup is required</td>
</tr>
</tbody></table>
<p><strong>🧮 Example</strong></p>
<p>Let’s say the CPU wants to access virtual address <code>0x00403ABC</code>:</p>
<ul>
<li>VPN &#x3D; top bits of address → check if TLB has entry</li>
<li>If yes → get PPN and combine with offset → access physical memory</li>
<li>If no → consult page table → update TLB → then access memory</li>
</ul>
<p><strong>🔄 TLB Replacement Policy</strong></p>
<p>When the TLB is full, and a new entry must be loaded, a <strong>replacement policy</strong> is used, such as:</p>
<ul>
<li><strong>Least Recently Used (LRU)</strong></li>
<li><strong>Random replacement</strong></li>
</ul>
<p><strong>🚀 Summary</strong> </p>
<ul>
<li>TLB significantly <strong>improves performance</strong> of memory access.</li>
<li>Acts as a <strong>fast cache</strong> for recent address translations.</li>
<li>Helps bridge the speed gap between the CPU and memory systems.</li>
</ul>
<hr>
<h3 id="Page-Table"><a href="#Page-Table" class="headerlink" title="Page Table"></a>Page Table</h3><p>In computer architecture, a <strong>Page Table</strong> is a data structure used by the <strong>virtual memory system</strong> to manage the mapping between <strong>virtual addresses</strong> and <strong>physical addresses</strong>.</p>
<p><strong>🧠 What is Virtual Memory?</strong></p>
<p>Virtual memory allows a program to use a large, continuous address space even if the physical memory (RAM) is smaller. The CPU generates <strong>virtual addresses</strong>, which must be translated into <strong>physical addresses</strong> before accessing actual memory.</p>
<p><strong>📘 What is a Page Table?</strong></p>
<p>A <strong>Page Table</strong> stores the mapping between <strong>virtual pages</strong> and <strong>physical frames</strong>. Each entry in the page table is called a <strong>Page Table Entry (PTE)</strong> and contains information such as:</p>
<ul>
<li><strong>Frame Number</strong>: the physical frame corresponding to the virtual page.</li>
<li><strong>Valid&#x2F;Invalid Bit</strong>: indicates if the mapping is valid.</li>
<li><strong>Protection Bits</strong>: control read&#x2F;write permissions.</li>
<li><strong>Dirty Bit</strong>: indicates if the page has been modified.</li>
<li><strong>Accessed Bit</strong>: indicates if the page has been accessed recently.</li>
</ul>
<p><strong>🧾 Example Structure</strong></p>
<table>
<thead>
<tr>
<th>Virtual Page Number</th>
<th>Physical Frame Number</th>
<th>Valid Bit</th>
<th>Protection</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>5</td>
<td>1</td>
<td>Read&#x2F;Write</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>1</td>
<td>Read-only</td>
</tr>
<tr>
<td>2</td>
<td>-</td>
<td>0</td>
<td>-</td>
</tr>
</tbody></table>
<p><strong>🧮 Address Translation Steps</strong></p>
<ol>
<li><p>The CPU generates a <strong>virtual address</strong>.</p>
</li>
<li><p>The virtual address is divided into:</p>
<ul>
<li><strong>Page Number</strong></li>
<li><strong>Offset</strong></li>
</ul>
</li>
<li><p>The <strong>Page Number</strong> is used to index into the Page Table.</p>
</li>
<li><p>The Page Table Entry provides the <strong>Frame Number</strong>.</p>
</li>
<li><p>The <strong>Physical Address</strong> is formed by combining the Frame Number and the Offset.</p>
</li>
</ol>
<p><strong>🛑 Page Fault</strong></p>
<p>If the <strong>Valid Bit</strong> is 0, the page is not currently in memory. This triggers a <strong>Page Fault</strong>, and the operating system must load the page from disk into RAM.</p>
<p><strong>🧭 Types of Page Tables</strong></p>
<ul>
<li><strong>Single-Level Page Table</strong>: Simple but not scalable for large address spaces.</li>
<li><strong>Multi-Level Page Table</strong>: Reduces memory usage by using a tree-like structure.</li>
<li><strong>Inverted Page Table</strong>: Indexes by physical frame instead of virtual page.</li>
<li><strong>Hashed Page Table</strong>: Used in systems with large address spaces (like 64-bit).</li>
</ul>
<p><strong>📌 Summary</strong></p>
<ul>
<li>Page Tables are essential for translating virtual to physical addresses.</li>
<li>They enable <strong>memory protection</strong>, <strong>process isolation</strong>, and <strong>efficient memory use</strong>.</li>
<li>Optimized with techniques like <strong>TLB</strong> (Translation Lookaside Buffer) to speed up address translation.</li>
</ul>
<img src="/img/concepts/tlb.svg" srcset="/img/loading.gif" lazyload alt="虚实地址转换" style="max-width: 100%; height: auto;" />

<hr>
<h3 id="🧩-3-Types-of-Cache-Misses"><a href="#🧩-3-Types-of-Cache-Misses" class="headerlink" title="🧩 3 Types of Cache Misses"></a>🧩 3 Types of Cache Misses</h3><p>In computer architecture, a <strong>cache miss</strong> occurs when the data requested by the CPU is <strong>not found in the cache</strong>, requiring access to a slower memory level (like RAM). There are <strong>three main types</strong> of cache misses:</p>
<p><strong>🧠 Summary Table</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Cause</th>
<th>Possible Solution</th>
</tr>
</thead>
<tbody><tr>
<td>Compulsory Miss</td>
<td>First-time access to a block</td>
<td>Prefetching, larger block size</td>
</tr>
<tr>
<td>Capacity Miss</td>
<td>Cache too small for working set</td>
<td>Larger cache, better algorithms</td>
</tr>
<tr>
<td>Conflict Miss</td>
<td>Blocks map to the same cache location</td>
<td>Higher associativity, alignment</td>
</tr>
</tbody></table>
<hr>
<h3 id="1-Compulsory-Miss-a-k-a-Cold-Miss"><a href="#1-Compulsory-Miss-a-k-a-Cold-Miss" class="headerlink" title="1. Compulsory Miss (a.k.a. Cold Miss)"></a>1. <strong>Compulsory Miss</strong> (a.k.a. Cold Miss)</h3><p><strong>🔹 What is it?</strong></p>
<p>Occurs the <strong>first time</strong> a block is accessed and <strong>has never been loaded into the cache</strong> before.</p>
<p><strong>🧠 Cause:</strong></p>
<ul>
<li>Cache is empty or data is accessed for the first time.</li>
</ul>
<p><strong>📌 Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-text" data-language="text"><code class="language-text">Accessing array A[0] for the first time → not in cache → compulsory miss<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p><strong>✅ Solution:</strong></p>
<ul>
<li>Prefetching</li>
<li>Larger block size (to bring in more adjacent data)</li>
</ul>
<hr>
<h3 id="2-Capacity-Miss"><a href="#2-Capacity-Miss" class="headerlink" title="2. Capacity Miss"></a>2. <strong>Capacity Miss</strong></h3><p><strong>🔹 What is it?</strong></p>
<p>Happens when the <strong>cache cannot contain all the needed data</strong>, and a previously loaded block gets evicted due to <strong>limited size</strong>.</p>
<p><strong>🧠 Cause:</strong></p>
<ul>
<li>The working set is <strong>larger than the cache</strong>.</li>
</ul>
<p><strong>📌 Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-text" data-language="text"><code class="language-text">Looping through a large dataset that exceeds cache size → older blocks are evicted → miss<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p><strong>✅ Solution:</strong></p>
<ul>
<li>Increase cache size</li>
<li>Optimize algorithm locality</li>
</ul>
<hr>
<h3 id="3-Conflict-Miss-a-k-a-Collision-Miss"><a href="#3-Conflict-Miss-a-k-a-Collision-Miss" class="headerlink" title="3. Conflict Miss (a.k.a. Collision Miss)"></a><strong>3. Conflict Miss</strong> (a.k.a. Collision Miss)</h3><p><strong>🔹 What is it?</strong></p>
<p>Occurs when <strong>multiple blocks map to the same cache line</strong> (in set-associative or direct-mapped caches), causing <strong>unnecessary evictions</strong>.</p>
<p><strong>🧠 Cause:</strong></p>
<ul>
<li>Limited associativity in the cache</li>
<li>Poor address mapping</li>
</ul>
<p><strong>📌 Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-text" data-language="text"><code class="language-text">Accessing addresses A and B that both map to cache set 3 → one evicts the other → conflict miss<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p><strong>✅ Solution:</strong></p>
<ul>
<li>Increase associativity</li>
<li>Use fully associative cache</li>
<li>Better hash functions or memory alignment</li>
</ul>
<hr>
<h3 id="🔗-Four-Types-of-Data-Dependencies-in-Computer-Architecture"><a href="#🔗-Four-Types-of-Data-Dependencies-in-Computer-Architecture" class="headerlink" title="🔗 Four Types of Data Dependencies in Computer Architecture"></a>🔗 Four Types of Data Dependencies in Computer Architecture</h3><p>In pipelined processors, <strong>data dependencies</strong> occur when instructions depend on the results of previous instructions. These dependencies can cause <strong>pipeline hazards</strong> and impact instruction-level parallelism.</p>
<p>There are <strong>four types</strong> of data dependencies:</p>
<p><strong>📊 Summary Table</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Abbreviation</th>
<th>Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody><tr>
<td>Flow Dependency</td>
<td>RAW</td>
<td>Read after write</td>
<td>Forwarding, Stall</td>
</tr>
<tr>
<td>Anti Dependency</td>
<td>WAR</td>
<td>Write after read</td>
<td>Register renaming</td>
</tr>
<tr>
<td>Output Dependency</td>
<td>WAW</td>
<td>Write after write</td>
<td>Register renaming, in-order commit</td>
</tr>
<tr>
<td>Control Dependency</td>
<td>–</td>
<td>Depends on branch outcome</td>
<td>Branch prediction, speculation</td>
</tr>
</tbody></table>
<hr>
<h3 id="1-Flow-Dependency-Read-After-Write-RAW"><a href="#1-Flow-Dependency-Read-After-Write-RAW" class="headerlink" title="1. Flow Dependency (Read After Write, RAW)"></a>1. <strong>Flow Dependency</strong> (Read After Write, <strong>RAW</strong>)</h3><p><strong>🔹 What is it?</strong></p>
<p>Occurs when an instruction needs to <strong>read a value</strong> that has not yet been <strong>written</strong> by a previous instruction.</p>
<p><strong>🧠 Cause:</strong></p>
<ul>
<li>The second instruction <strong>depends</strong> on the result of the first.</li>
</ul>
<p><strong>📌 Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-asm" data-language="asm"><code class="language-asm">I1: R1 ← R2 + R3
I2: R4 ← R1 + R5   ; RAW: I2 reads R1 before I1 writes it<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>

<p><strong>✅ Solution:</strong></p>
<ul>
<li><strong>Forwarding&#x2F;Bypassing</strong></li>
<li><strong>Stalling</strong> the pipeline</li>
</ul>
<hr>
<h3 id="2-Anti-Dependency-Write-After-Read-WAR"><a href="#2-Anti-Dependency-Write-After-Read-WAR" class="headerlink" title="2. Anti Dependency (Write After Read, WAR)"></a>2. <strong>Anti Dependency</strong> (Write After Read, <strong>WAR</strong>)</h3><p><strong>🔹 What is it?</strong></p>
<p>Occurs when a later instruction <strong>writes</strong> to a location that a previous instruction still needs to <strong>read</strong> from.</p>
<p><strong>🧠 Cause:</strong></p>
<ul>
<li>The second instruction must <strong>not overwrite</strong> the value too early.</li>
</ul>
<p><strong>📌 Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-asm" data-language="asm"><code class="language-asm">I1: R4 ← R1 + R2   ; Reads R1
I2: R1 ← R3 + R5   ; WAR: I2 writes R1 before I1 finishes reading<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>

<p><strong>✅ Solution:</strong></p>
<ul>
<li><strong>Register renaming</strong></li>
</ul>
<hr>
<h3 id="3-Output-Dependency-Write-After-Write-WAW"><a href="#3-Output-Dependency-Write-After-Write-WAW" class="headerlink" title="3. Output Dependency (Write After Write, WAW)"></a>3. <strong>Output Dependency</strong> (Write After Write, <strong>WAW</strong>)</h3><p><strong>🔹 What is it?</strong></p>
<p>Occurs when two instructions <strong>write</strong> to the <strong>same destination</strong>, and the final result must reflect the correct write order.</p>
<p><strong>🧠 Cause:</strong></p>
<ul>
<li>Later instruction must not overwrite an earlier write <strong>out of order</strong>.</li>
</ul>
<p><strong>📌 Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-asm" data-language="asm"><code class="language-asm">I1: R1 ← R2 + R3
I2: R1 ← R4 + R5   ; WAW: I2 writes R1 before I1 completes<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>

<p><strong>✅ Solution:</strong></p>
<ul>
<li><strong>Register renaming</strong></li>
<li><strong>In-order commit</strong> in out-of-order execution</li>
</ul>
<hr>
<h3 id="4-Control-Dependency"><a href="#4-Control-Dependency" class="headerlink" title="4. Control Dependency"></a>4. <strong>Control Dependency</strong></h3><p><strong>🔹 What is it?</strong></p>
<p>Occurs when the <strong>execution</strong> of an instruction depends on the <strong>outcome of a branch</strong>.</p>
<p><strong>🧠 Cause:</strong></p>
<ul>
<li>It’s not clear <strong>whether to execute</strong> the instruction until the branch is resolved.</li>
</ul>
<p><strong>📌 Example:</strong></p>
<figure><div class="code-wrapper"><pre class="line-numbers language-asm" data-language="asm"><code class="language-asm">I1: if (R1 &#x3D;&#x3D; 0) goto LABEL
I2: R2 ← R3 + R4   ; Control dependent on I1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div></figure>

<p><strong>✅ Solution:</strong></p>
<ul>
<li><strong>Branch prediction</strong></li>
<li><strong>Speculative execution</strong></li>
</ul>
<hr>
<h3 id="Microprogramming（微程序设计）"><a href="#Microprogramming（微程序设计）" class="headerlink" title="Microprogramming（微程序设计）"></a>Microprogramming（微程序设计）</h3><p><strong>Microprogramming</strong> is a method of implementing a CPU’s control unit using <strong>a sequence of microinstructions</strong> stored in a special memory called <strong>control memory</strong>.<br>Each <strong>microinstruction</strong> specifies low-level operations like setting control signals, reading&#x2F;writing registers, or ALU actions.<br>It acts as an intermediate layer between machine instructions and hardware control signals.<br>There are two types of control units: <strong>hardwired control</strong> and <strong>microprogrammed control</strong> — the latter is easier to modify and extend.<br>Microprogramming was widely used in classic CISC architectures like IBM System&#x2F;360.</p>
<hr>
<h3 id="Snoopy-Cache（监听式缓存）"><a href="#Snoopy-Cache（监听式缓存）" class="headerlink" title="Snoopy Cache（监听式缓存）"></a>Snoopy Cache（监听式缓存）</h3><p><strong>Snoopy Cache</strong> is a cache coherence protocol used in <strong>multiprocessor systems</strong> to maintain data consistency among multiple caches.<br>Each cache monitors (or “snoops”) a shared communication bus to detect if other processors are reading or writing a memory block it has cached.<br>If a write is detected, the snooping cache can <strong>invalidate</strong> or <strong>update</strong> its own copy to keep data coherent.<br>This ensures that all processors work with the <strong>most recent version</strong> of data.<br>Common snoopy-based protocols include <strong>MSI</strong>, <strong>MESI</strong>, and <strong>MOESI</strong>.</p>
<p><strong>Snoopy Cache（监听式缓存）</strong>是一种用于<strong>多处理器系统</strong>的缓存一致性协议，用于保持多个缓存之间的数据一致性。<br>每个缓存会监听（snoop）共享总线，检测其他处理器是否在读取或写入它缓存中的数据块。<br>当检测到写操作时，监听缓存会<strong>更新</strong>或<strong>无效化</strong>自己的副本，以保持数据的一致性。<br>这样可以确保所有处理器使用的都是<strong>最新版本的数据</strong>。<br>常见的监听式协议包括 <strong>MSI</strong>、<strong>MESI</strong> 和 <strong>MOESI</strong> 协议。</p>
<hr>
<h3 id="Out-of-order-Execution-乱序执行"><a href="#Out-of-order-Execution-乱序执行" class="headerlink" title="Out-of-order Execution (乱序执行)"></a>Out-of-order Execution (乱序执行)</h3><p><strong>Out-of-order execution</strong> is a technique used in modern processors to improve performance by allowing instructions to be executed <strong>out of the order they appear</strong> in the program.<br>Instead of waiting for earlier instructions to complete (which might be delayed due to data hazards or memory stalls), the CPU executes instructions that are ready and independent of others.<br>This reduces <strong>idle CPU time</strong> and <strong>increases throughput</strong> by utilizing all available execution units.<br>Dependencies between instructions are tracked, and results are committed in the correct order.<br>Out-of-order execution is common in <strong>superscalar processors</strong> and is part of <strong>dynamic instruction scheduling</strong>.</p>
<p><strong>乱序执行</strong>是一种现代处理器中常用的技术，通过允许指令<strong>按非顺序</strong>的方式执行来提升性能。<br>CPU 不必等待前面的指令完成（这些指令可能因为数据依赖或内存延迟而被拖慢），而是执行那些准备好且相互独立的指令。<br>这样可以减少<strong>CPU空闲时间</strong>并<strong>增加吞吐量</strong>，充分利用所有可用的执行单元。<br>指令之间的依赖关系会被追踪，结果会按正确的顺序提交。<br>乱序执行在<strong>超标量处理器</strong>中很常见，并且是<strong>动态指令调度</strong>的一部分。</p>
<hr>
<h3 id="DMA-Direct-Memory-Access"><a href="#DMA-Direct-Memory-Access" class="headerlink" title="DMA (Direct Memory Access)"></a>DMA (Direct Memory Access)</h3><p><strong>DMA</strong> stands for <strong>Direct Memory Access</strong>, a feature that allows certain hardware components (like disk controllers or network cards) to <strong>transfer data directly to&#x2F;from main memory</strong> without involving the CPU.<br>This improves system efficiency by <strong>freeing the CPU</strong> from managing large or repetitive data transfers.<br>A <strong>DMA controller</strong> handles the data movement and signals the CPU when the transfer is complete.<br>DMA is commonly used for tasks like <strong>disk I&#x2F;O</strong>, <strong>audio&#x2F;video streaming</strong>, and <strong>network communication</strong>.<br>It helps achieve <strong>high-speed data transfer</strong> with minimal CPU overhead.</p>
<p><strong>DMA</strong> 是 <strong>直接内存访问（Direct Memory Access）</strong> 的缩写，是一种允许某些硬件组件（如磁盘控制器或网卡）<strong>在不经过 CPU 的情况下直接与主存进行数据传输</strong>的技术。<br>它通过<strong>减轻 CPU 的数据搬运负担</strong>，提高了系统效率。<br>数据传输由一个 <strong>DMA 控制器</strong>负责，传输完成后会通知 CPU。<br>DMA 广泛应用于 <strong>磁盘 I&#x2F;O</strong>、<strong>音视频流处理</strong> 和 <strong>网络通信</strong> 等领域。<br>它能以<strong>极低的 CPU 开销实现高速数据传输</strong>。</p>
<hr>
<h3 id="RISC-Reduced-Instruction-Set-Computing"><a href="#RISC-Reduced-Instruction-Set-Computing" class="headerlink" title="RISC (Reduced Instruction Set Computing)"></a>RISC (Reduced Instruction Set Computing)</h3><p><strong>RISC</strong> stands for <strong>Reduced Instruction Set Computing</strong>, a CPU design strategy that uses a <strong>small set of simple and fast instructions</strong>.<br>Each instruction typically executes in <strong>one clock cycle</strong>, allowing efficient pipelining and parallelism.<br>RISC emphasizes <strong>hardware simplicity</strong>, <strong>fixed instruction length</strong>, and <strong>a load&#x2F;store architecture</strong>.<br>It is well-suited for modern compilers and high-performance applications.<br>Examples: <strong>ARM</strong>, <strong>RISC-V</strong>, <strong>MIPS</strong>.</p>
<p><strong>RISC</strong> 是 <strong>精简指令集计算（Reduced Instruction Set Computing）</strong> 的缩写，是一种使用<strong>少量简单指令</strong>的 CPU 设计理念。<br>每条指令通常在<strong>一个时钟周期内完成</strong>，便于实现流水线和并行处理。<br>RISC 强调<strong>硬件简化</strong>、<strong>固定长度指令</strong>以及<strong>Load&#x2F;Store 架构</strong>。<br>这种架构非常适合现代编译器和高性能应用。<br>代表架构：<strong>ARM</strong>、<strong>RISC-V</strong>、<strong>MIPS</strong>。</p>
<hr>
<h3 id="CISC-Complex-Instruction-Set-Computing"><a href="#CISC-Complex-Instruction-Set-Computing" class="headerlink" title="CISC (Complex Instruction Set Computing)"></a>CISC (Complex Instruction Set Computing)</h3><p><strong>CISC</strong> stands for <strong>Complex Instruction Set Computing</strong>, which uses a <strong>large and versatile set of instructions</strong>, where some instructions perform <strong>multi-step operations</strong>.<br>This design reduces the number of instructions per program but increases the complexity of the CPU.<br>CISC instructions often have <strong>variable lengths</strong> and require <strong>multiple clock cycles</strong> to execute.<br>It was originally designed to minimize memory usage and support simpler compilers.<br>Example: <strong>x86 architecture</strong> (Intel, AMD).</p>
<p><strong>CISC</strong> 是 <strong>复杂指令集计算（Complex Instruction Set Computing）</strong> 的缩写，采用<strong>数量多、功能强的指令集</strong>，其中一些指令能完成<strong>多个低层次操作</strong>。<br>这种设计可以减少程序中所需的指令数量，但会增加 CPU 的实现复杂度。<br>CISC 指令通常是<strong>不固定长度</strong>，并且执行时<strong>可能需要多个时钟周期</strong>。<br>它最初为了减少内存使用、便于早期编译器设计而提出。<br>代表架构：<strong>x86 架构</strong>（如 Intel 和 AMD 处理器）。</p>
<hr>
<h3 id="SIMD-Single-Instruction-Multiple-Data"><a href="#SIMD-Single-Instruction-Multiple-Data" class="headerlink" title="SIMD (Single Instruction, Multiple Data)"></a>SIMD (Single Instruction, Multiple Data)</h3><p><strong>SIMD</strong> stands for <strong>Single Instruction, Multiple Data</strong>, a parallel computing model where <strong>one instruction</strong> operates on <strong>multiple data elements simultaneously</strong>.<br>It is especially useful for tasks like <strong>image processing</strong>, <strong>audio&#x2F;video encoding</strong>, <strong>machine learning</strong>, and <strong>scientific computing</strong>.<br>SIMD improves performance by exploiting <strong>data-level parallelism</strong>.<br>Modern CPUs include SIMD instruction sets such as <strong>SSE</strong>, <strong>AVX</strong> (Intel&#x2F;AMD), and <strong>NEON</strong> (ARM).<br>It is a core concept in <strong>vector processing</strong> and used in <strong>GPUs</strong> as well.</p>
<p><strong>SIMD</strong> 是 <strong>单指令多数据流（Single Instruction, Multiple Data）</strong> 的缩写，是一种并行计算模型，其中<strong>一条指令</strong>可以同时作用于<strong>多个数据元素</strong>。<br>它非常适用于图像处理、音视频编解码、机器学习和科学计算等场景。<br>SIMD 通过挖掘数据级并行性（Data-level Parallelism）来提升性能。<br>现代 CPU 提供了 SIMD 指令集，如 <strong>SSE</strong>、<strong>AVX</strong>（Intel&#x2F;AMD）和 <strong>NEON</strong>（ARM）。<br>它是向量处理（Vector Processing）的核心思想，并广泛用于 <strong>GPU</strong> 中。</p>
<h2 id="4-Formal-Language-Automata"><a href="#4-Formal-Language-Automata" class="headerlink" title="4. Formal Language &amp; Automata"></a>4. Formal Language &amp; Automata</h2><h3 id="Regular-Language"><a href="#Regular-Language" class="headerlink" title="Regular Language"></a>Regular Language</h3><p>A <strong>Regular Language</strong> is a class of formal languages that can be <strong>recognized by finite automata</strong>. It can be described by:</p>
<ul>
<li><strong>Deterministic Finite Automata (DFA)</strong></li>
<li><strong>Non-deterministic Finite Automata (NFA)</strong></li>
<li><strong>Regular expressions</strong></li>
</ul>
<p>If a language can be represented using any of the above methods, it is a <strong>regular language</strong>.</p>
<p><strong>🔹 Examples</strong></p>
<ol>
<li><p>Language of all strings over <code>&#123;a, b&#125;</code> that contain <strong>only</strong> <code>a</code>:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">L &#x3D; &#123; ε, a, aa, aaa, ... &#125; &#x3D; a*<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>
</li>
<li><p>Language of all strings that start with <code>a</code> and end with <code>b</code>:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">L &#x3D; a(a|b)*b<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>
</li>
<li><p>Language of all strings with even number of <code>0</code>s over <code>&#123;0,1&#125;</code>:</p>
<ul>
<li>This can be recognized by a DFA with two states toggling on <code>0</code>.</li>
</ul>
</li>
</ol>
<ul>
<li><p>They can be <strong>expressed using regular expressions</strong>, and there is an equivalence between:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">DFA ⇄ NFA ⇄ Regular Expression<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure></li>
</ul>
<hr>
<h3 id="Non-Regular-Languages"><a href="#Non-Regular-Languages" class="headerlink" title="Non-Regular Languages"></a>Non-Regular Languages</h3><p>Some languages <strong>cannot</strong> be described by regular expressions or finite automata. For example:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">L &#x3D; &#123; aⁿbⁿ | n ≥ 0 &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p>This language requires memory to keep track of the number of <code>a</code>s and <code>b</code>s, which finite automata do not have.</p>
<p>To prove that a language is <strong>not regular</strong>, we can use the <strong>Pumping Lemma</strong>, which states:</p>
<p>If a language <code>L</code> is regular, there exists an integer <code>p &gt; 0</code> (pumping length) such that any string <code>s ∈ L</code> with length ≥ <code>p</code> can be split into <code>s = xyz</code> such that:</p>
<ul>
<li>|y| &gt; 0</li>
<li>|xy| ≤ p</li>
<li>For all <code>i ≥ 0</code>, the string <code>xyⁱz ∈ L</code></li>
</ul>
<p>If no such decomposition exists for a string, the language is <strong>not regular</strong>.</p>
<hr>
<h3 id="Closure-Properties-of-Regular-Languages"><a href="#Closure-Properties-of-Regular-Languages" class="headerlink" title="Closure Properties of Regular Languages"></a>Closure Properties of Regular Languages</h3><blockquote>
<p>🔹 What is “Closure”?</p>
</blockquote>
<p>In automata theory, a class of languages is said to be <strong>closed under an operation</strong> if <strong>applying that operation</strong> to languages in the class <strong>results in a language that is also in the class</strong>.</p>
<p>For <strong>regular languages</strong>, they are closed under many operations. This means if you take <strong>regular languages</strong> and apply these operations, the result will <strong>still be a regular language</strong>.</p>
<blockquote>
<p>🔹 Closure Under Common Operations</p>
</blockquote>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Description</th>
<th>Result</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Union</strong></td>
<td>If L₁ and L₂ are regular, then L₁ ∪ L₂ is also regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Intersection</strong></td>
<td>If L₁ and L₂ are regular, then L₁ ∩ L₂ is also regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Complement</strong></td>
<td>If L is regular, then its complement ¬L is also regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Concatenation</strong></td>
<td>If L₁ and L₂ are regular, then L₁L₂ is also regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Kleene Star</strong></td>
<td>If L is regular, then L* (zero or more repetitions) is also regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Reversal</strong></td>
<td>If L is regular, then the reverse of L is also regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Difference</strong></td>
<td>If L₁ and L₂ are regular, then L₁ - L₂ is also regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Homomorphism</strong></td>
<td>Applying a homomorphism to a regular language gives a regular language.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Inverse Homomorphism</strong></td>
<td>Inverse images of regular languages under homomorphisms are regular.</td>
<td>✅ Regular</td>
</tr>
<tr>
<td><strong>Substitution</strong></td>
<td>Regular languages are closed under substitution with regular languages.</td>
<td>✅ Regular</td>
</tr>
</tbody></table>
<blockquote>
<p>🔹 Example: Union Closure</p>
</blockquote>
<p>Let:</p>
<ul>
<li>L₁ &#x3D; { w | w contains only a’s } &#x3D; <code>a*</code></li>
<li>L₂ &#x3D; { w | w contains only b’s } &#x3D; <code>b*</code></li>
</ul>
<p>Then:</p>
<ul>
<li>L₁ ∪ L₂ &#x3D; { w | w contains only a’s or only b’s } &#x3D; <code>a* ∪ b*</code></li>
<li>This is still a regular language.</li>
</ul>
<blockquote>
<p>🔹 Why Closure is Useful</p>
</blockquote>
<ul>
<li>Closure properties help us <strong>construct complex regular languages</strong> from simpler ones.</li>
<li>They are essential in <strong>proving properties</strong> of languages.</li>
<li>They are used in <strong>compiler design</strong>, <strong>regex engines</strong>, and <strong>automated verification</strong>.</li>
</ul>
<hr>
<h3 id="🤖-DFA-Deterministic-Finite-Automaton"><a href="#🤖-DFA-Deterministic-Finite-Automaton" class="headerlink" title="🤖 DFA (Deterministic Finite Automaton)"></a>🤖 DFA (Deterministic Finite Automaton)</h3><p><strong>🔹 Definition</strong></p>
<p>A <strong>DFA</strong> is a type of <strong>finite automaton</strong> used to <strong>recognize regular languages</strong>. It reads an input string <strong>symbol by symbol</strong>, and <strong>at any point</strong> in time, it is in <strong>exactly one state</strong>.</p>
<p>A DFA is defined as a <strong>5-tuple</strong>:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">DFA &#x3D; (Q, Σ, δ, q₀, F)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p>Where:</p>
<ul>
<li><code>Q</code> → Finite set of <strong>states</strong></li>
<li><code>Σ</code> → Finite set of <strong>input symbols</strong> (alphabet)</li>
<li><code>δ</code> → <strong>Transition function</strong>: δ : Q × Σ → Q</li>
<li><code>q₀</code> → <strong>Start state</strong>, where the computation begins (<code>q₀ ∈ Q</code>)</li>
<li><code>F</code> → Set of <strong>accepting&#x2F;final states</strong> (<code>F ⊆ Q</code>)</li>
</ul>
<p><strong>🔹 Characteristics</strong></p>
<ul>
<li><strong>Deterministic</strong>: For every state <code>q ∈ Q</code> and every input symbol <code>a ∈ Σ</code>, <strong>there is exactly one transition</strong> defined:<br>δ(q, a) &#x3D; q’</li>
<li><strong>No epsilon (ε) transitions</strong>: Input must be read <strong>one symbol at a time</strong></li>
<li>DFA <strong>always knows</strong> what to do next</li>
</ul>
<p><strong>🔹 Example DFA</strong></p>
<p>Let’s define a DFA that accepts all strings over <code>&#123;0,1&#125;</code> that end with <code>01</code>.</p>
<ul>
<li><code>Q = &#123;q0, q1, q2&#125;</code></li>
<li><code>Σ = &#123;0, 1&#125;</code></li>
<li><code>q₀ = q0</code></li>
<li><code>F = &#123;q2&#125;</code></li>
</ul>
<p>Transition table:</p>
<table>
<thead>
<tr>
<th>Current State</th>
<th>Input</th>
<th>Next State</th>
</tr>
</thead>
<tbody><tr>
<td>q0</td>
<td>0</td>
<td>q1</td>
</tr>
<tr>
<td>q0</td>
<td>1</td>
<td>q0</td>
</tr>
<tr>
<td>q1</td>
<td>0</td>
<td>q1</td>
</tr>
<tr>
<td>q1</td>
<td>1</td>
<td>q2</td>
</tr>
<tr>
<td>q2</td>
<td>0</td>
<td>q1</td>
</tr>
<tr>
<td>q2</td>
<td>1</td>
<td>q0</td>
</tr>
</tbody></table>
<p>This DFA accepts strings like: <code>01</code>, <code>1001</code>, <code>1101</code>, etc.</p>
<p><strong>🔹 DFA vs NFA</strong></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>DFA</th>
<th>NFA</th>
</tr>
</thead>
<tbody><tr>
<td>Transition</td>
<td>One unique next state</td>
<td>May have multiple next states</td>
</tr>
<tr>
<td>ε-transitions</td>
<td>Not allowed</td>
<td>Allowed</td>
</tr>
<tr>
<td>Simplicity</td>
<td>Easier to implement</td>
<td>Easier to design</td>
</tr>
<tr>
<td>Power</td>
<td><strong>Same (both recognize regular languages)</strong></td>
<td></td>
</tr>
</tbody></table>
<p><strong>🔹 Applications</strong></p>
<ul>
<li>Lexical analysis (token recognition)</li>
<li>Pattern matching (e.g., <code>grep</code>, <code>regex</code>)</li>
<li>Protocol design</li>
<li>Digital circuits</li>
</ul>
<hr>
<h3 id="🔄-NFA-Nondeterministic-Finite-Automaton"><a href="#🔄-NFA-Nondeterministic-Finite-Automaton" class="headerlink" title="🔄 NFA (Nondeterministic Finite Automaton)"></a>🔄 NFA (Nondeterministic Finite Automaton)</h3><p><strong>🔹 Definition</strong></p>
<p>An <strong>NFA</strong> is a type of <strong>finite automaton</strong> used to recognize <strong>regular languages</strong>, just like a DFA. The key difference is that <strong>multiple transitions</strong> are allowed for the <strong>same input</strong> from a single state, and <strong>ε-transitions</strong> (transitions without consuming input) are permitted.</p>
<p>An NFA is formally defined as a <strong>5-tuple</strong>:</p>
<figure><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">NFA &#x3D; (Q, Σ, δ, q₀, F)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure>

<p>Where:</p>
<ul>
<li><code>Q</code> → Finite set of <strong>states</strong></li>
<li><code>Σ</code> → Finite set of <strong>input symbols</strong> (alphabet)</li>
<li><code>δ</code> → <strong>Transition function</strong>: δ : Q × (Σ ∪ {ε}) → 2^Q</li>
<li><code>q₀</code> → <strong>Start state</strong>, <code>q₀ ∈ Q</code></li>
<li><code>F</code> → Set of <strong>accepting&#x2F;final states</strong>, <code>F ⊆ Q</code></li>
</ul>
<p><strong>🔹 Characteristics</strong></p>
<ul>
<li><p><strong>Non-deterministic</strong>:</p>
<ul>
<li>From a given state and input, the NFA can <strong>go to multiple next states</strong>.</li>
<li>The NFA <strong>accepts</strong> an input string if <strong>at least one possible path</strong> leads to an accepting state.</li>
</ul>
</li>
<li><p><strong>ε-transitions allowed</strong>: The machine can move <strong>without reading input</strong>.</p>
</li>
<li><p><strong>Can be in multiple states</strong> at once during computation.</p>
</li>
</ul>
<p><strong>🔹 Example NFA</strong></p>
<p>Let’s define an NFA that accepts strings over <code>&#123;0,1&#125;</code> ending in <code>01</code>.</p>
<ul>
<li><code>Q = &#123;q0, q1, q2&#125;</code></li>
<li><code>Σ = &#123;0, 1&#125;</code></li>
<li><code>q₀ = q0</code></li>
<li><code>F = &#123;q2&#125;</code></li>
</ul>
<p>Transition table:</p>
<table>
<thead>
<tr>
<th>Current State</th>
<th>Input</th>
<th>Next States</th>
</tr>
</thead>
<tbody><tr>
<td>q0</td>
<td>0</td>
<td>{q0, q1}</td>
</tr>
<tr>
<td>q0</td>
<td>1</td>
<td>{q0}</td>
</tr>
<tr>
<td>q1</td>
<td>1</td>
<td>{q2}</td>
</tr>
<tr>
<td>q2</td>
<td>0,1</td>
<td>∅</td>
</tr>
</tbody></table>
<p>In this NFA, from <code>q0</code>, if we read <code>0</code>, we can <strong>either</strong> stay in <code>q0</code> or go to <code>q1</code>.</p>
<p><strong>🔹 NFA vs DFA</strong></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>NFA</th>
<th>DFA</th>
</tr>
</thead>
<tbody><tr>
<td>Transitions</td>
<td>Can go to <strong>multiple</strong> states</td>
<td>Only <strong>one</strong> next state</td>
</tr>
<tr>
<td>ε-transitions</td>
<td><strong>Allowed</strong></td>
<td><strong>Not allowed</strong></td>
</tr>
<tr>
<td>Execution</td>
<td>Can explore multiple paths in parallel</td>
<td>Only one deterministic path</td>
</tr>
<tr>
<td>Implementation</td>
<td>More complex</td>
<td>Easier</td>
</tr>
<tr>
<td>Expressive Power</td>
<td><strong>Same</strong> – both recognize regular languages</td>
<td></td>
</tr>
</tbody></table>
<blockquote>
<p>💡 Every NFA can be converted into an equivalent DFA (subset construction), though the DFA may have exponentially more states.</p>
</blockquote>
<p><strong>🔹 Applications</strong></p>
<ul>
<li>Easier to <strong>design</strong> than DFA for complex patterns</li>
<li>Used in <strong>regular expression engines</strong></li>
<li>Foundations for <strong>parser generators</strong> and <strong>compiler design</strong></li>
</ul>
<h2 id="5-Programming-Language"><a href="#5-Programming-Language" class="headerlink" title="5. Programming Language"></a>5. Programming Language</h2><h2 id="6-Machine-Learning"><a href="#6-Machine-Learning" class="headerlink" title="6. Machine Learning"></a>6. Machine Learning</h2><h2 id="7-Digital-Circuit"><a href="#7-Digital-Circuit" class="headerlink" title="7. Digital Circuit"></a>7. Digital Circuit</h2><h2 id="8-Computer-Networks"><a href="#8-Computer-Networks" class="headerlink" title="8. Computer Networks"></a>8. Computer Networks</h2><h3 id="Distributed-Hash-Table-DHT"><a href="#Distributed-Hash-Table-DHT" class="headerlink" title="Distributed Hash Table (DHT)"></a>Distributed Hash Table (DHT)</h3><p>A <strong>Distributed Hash Table (DHT)</strong> is a decentralized system that provides a lookup service similar to a hash table: it stores <strong>(key, value)</strong> pairs and allows efficient retrieval of the value given a key. Instead of relying on a central server, DHT distributes the data across a network of nodes (often used in <strong>peer-to-peer networks</strong> and <strong>IoT</strong> systems).</p>
<p>Key Features:</p>
<ul>
<li><strong>Scalable</strong>: Works well even with thousands of nodes.</li>
<li><strong>Fault-tolerant</strong>: Data is replicated, so the system can handle node failures.</li>
<li><strong>Efficient</strong>: Most DHTs (like Chord or Kademlia) can find data in <strong>O(log n)</strong> time.</li>
</ul>
<p>DHTs are often used in applications like <strong>BitTorrent</strong> and <strong>distributed file systems</strong>, and are relevant in wireless and digital communication contexts, which aligns well with your research interests in IoT.</p>
<img src="/img/concepts/dht.svg" srcset="/img/loading.gif" lazyload alt="DHT" style="max-width: 100%; height: auto;" />



<h2 id="9-Cryptography"><a href="#9-Cryptography" class="headerlink" title="9. Cryptography"></a>9. Cryptography</h2><h2 id="10-Digital-Signal-Processing"><a href="#10-Digital-Signal-Processing" class="headerlink" title="10. Digital Signal Processing"></a>10. Digital Signal Processing</h2><h2 id="11-Control-Engineering"><a href="#11-Control-Engineering" class="headerlink" title="11. Control Engineering"></a>11. Control Engineering</h2><h2 id="12-Software-Development"><a href="#12-Software-Development" class="headerlink" title="12. Software Development"></a>12. Software Development</h2><h2 id="13-Robotics"><a href="#13-Robotics" class="headerlink" title="13. Robotics"></a>13. Robotics</h2>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%B8%93%E4%B8%9A%E7%A7%91%E7%9B%AE%E7%AC%94%E8%AE%B0/" class="category-chain-item">专业科目笔记</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E4%BF%AE%E8%80%83/" class="print-no-link">#修考</a>
      
        <a href="/tags/CS-Concepts/" class="print-no-link">#CS Concepts</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>CS Concepts</div>
      <div>http://toutou.zeabur.app/2025/02/09/CS-Concepts/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>toutou</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>February 9, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - Non-commercial">
                    <i class="iconfont icon-cc-nc"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - Share-alike">
                    <i class="iconfont icon-cc-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/02/21/Calculus/" title="Calculus">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Calculus</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/02/09/computer-networks/" title="Computer Networks">
                        <span class="hidden-mobile">Computer Networks</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://www.instagram.com/hanni_rio/" target="_blank" rel="nofollow noopener"><span>Hanni Rio</span></a> <i class="iconfont icon-copyright"></i> <a href="https://toutou.pro/" target="_blank" rel="nofollow noopener"><span>toutou</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script  src="https://lib.baomitu.com/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js" ></script>

  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
